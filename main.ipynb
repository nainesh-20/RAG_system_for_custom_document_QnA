{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ed74acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported. Note: Ensure you have run `pip install ...` for the latest versions.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries\n",
    "\n",
    "# Basics\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import the core pinecone library (using the v3+ class)\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec # Or PodSpec if not serverless\n",
    "import time # Needed for waiting for index creation\n",
    "\n",
    "# LangChain Components (using modern import paths >= 0.1.0)\n",
    "\n",
    "# LLM (Use ChatOpenAI for most modern tasks)\n",
    "from langchain_openai import ChatOpenAI\n",
    "# If you specifically need the older completion endpoint:\n",
    "# from langchain_openai import OpenAI\n",
    "\n",
    "# Document Loader (correct path)\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Splitter (correct path)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Need tiktoken for accurate token counting with OpenAI models\n",
    "import tiktoken\n",
    "\n",
    "# Tokenizer (external library - correct path for transformers, if still needed)\n",
    "# from transformers import GPT2TokenizerFast # Keep if you specifically use this tokenizer later\n",
    "\n",
    "# Embedding (using modern import path)\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Vector Store (using modern import path for Pinecone)\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# For FAISS (location might vary slightly, but often in community):\n",
    "# from langchain_community.vectorstores import FAISS # If using FAISS locally\n",
    "\n",
    "# Imports for modern RAG chain construction (LCEL and utility chains)\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder # For building prompts\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel # For composing chains\n",
    "from langchain_core.output_parsers import StrOutputParser # For parsing output\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain # Utility chain constructors\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain # Needed for combining docs\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage # For handling chat history messages\n",
    "\n",
    "print(\"Libraries imported. Note: Ensure you have run `pip install ...` for the latest versions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d961b988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Environment Variables\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "# Make sure you have a .env file in your project root\n",
    "# with variables like:\n",
    "# OPENAI_API_KEY=\"your-openai-api-key\"\n",
    "# PINECONE_API_KEY=\"your-pinecone-api-key\"\n",
    "# PINECONE_ENVIRONMENT=\"your-pinecone-environment\" # e.g., \"us-east-1-aws\"\n",
    "# PINECONE_INDEX_NAME=\"your-index-name\"         # Your chosen index name\n",
    "\n",
    "# Imports (os, load_dotenv) already in Cell 1\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Get API keys and environment from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\")\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "\n",
    "# Basic check to ensure variables are loaded\n",
    "if not all([OPENAI_API_KEY, PINECONE_API_KEY, PINECONE_ENVIRONMENT, PINECONE_INDEX_NAME]):\n",
    "    raise ValueError(\"Ensure OPENAI_API_KEY, PINECONE_API_KEY, PINECONE_ENVIRONMENT, and PINECONE_INDEX_NAME are set in your environment variables or .env file\")\n",
    "\n",
    "print(\"Environment variables loaded successfully.\")\n",
    "# Avoid printing sensitive keys directly in notebook output for security\n",
    "# print(f\"OpenAI Key Loaded: {bool(OPENAI_API_KEY)}\")\n",
    "# print(f\"Pinecone Key Loaded: {bool(PINECONE_API_KEY)}\")\n",
    "# print(f\"Pinecone Environment: {PINECONE_ENVIRONMENT}\")\n",
    "# print(f\"Pinecone Index Name: {PINECONE_INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6255f19e",
   "metadata": {},
   "source": [
    "\n",
    "# PART 1: LANGCHAIN BASICS\n",
    "\n",
    "\n",
    "🎯 **Objective:** Understand what is the LangChain library and all the elements that are required to generate a simple pipeline to query out documents. \n",
    "\n",
    "### **What is LangChain?**\n",
    "> LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "LangChain makes the hardest parts of working with AI models easier in two main ways:\n",
    "\n",
    "1. **Data-aware** - Bring external data, such as your files, other applications, and API data, to your LLMs\n",
    "2. **Agentic** - Allow your LLMs to interact with it's environment via decision making. Use LLMs to help decide which action to take next. \n",
    "\n",
    "### **Why LangChain?**\n",
    "1. **Components** - Abstractions for working with language models, along with a collection of implementations for each abstraction. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not\n",
    "\n",
    "2. **Chains** - LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together. A structured assembly of components for accomplishing specific higher-level tasks.\n",
    "\n",
    "3. **Speed 🚢** - This team ships insanely fast. You'll be up to date with the latest LLM features.\n",
    "\n",
    "4. **Community 👥** - Wonderful discord and community support, meet ups, hackathons, etc.\n",
    "\n",
    "Though the usage of LLMs can be straightforward (text-in, text-out), when trying to build complex applications you'll quickly notice friction points. \n",
    "\n",
    "> LangChain helps with once you develop more complicated application and manage LLMs the way we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6565a345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ChatOpenAI model for basic test...\n",
      "ChatOpenAI model initialized.\n",
      "\n",
      "Sending prompt to LLM: 'Please, tell me some funny jokes'\n",
      "\n",
      "LLM Response:\n",
      "Sure, here are a few jokes for you:\n",
      "\n",
      "1. Why couldn't the bicycle stand up by itself? Because it was two tired!\n",
      "2. Why did the tomato turn red? Because it saw the salad dressing!\n",
      "3. What do you call a bear with no teeth? A gummy bear!\n",
      "4. Why did the math book look sad? Because it had too many problems.\n",
      "5. How does a penguin build its house? Igloos it together!\n",
      "\n",
      "I hope these jokes brought a smile to your face!\n"
     ]
    }
   ],
   "source": [
    "#  Test Basic LLM Call (using modern ChatOpenAI)\n",
    "\n",
    "# LLM (import already in Cell 1)\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Ensure OPENAI_API_KEY is loaded from Cell 2\n",
    "\n",
    "# Initialize the Chat LLM\n",
    "print(\"Initializing ChatOpenAI model for basic test...\")\n",
    "chatgpt = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",  # Specify the model name\n",
    "    temperature=0,          # Temperature controls randomness\n",
    "    openai_api_key=OPENAI_API_KEY # Pass the API key directly\n",
    ")\n",
    "print(\"ChatOpenAI model initialized.\")\n",
    "\n",
    "# Define a simple prompt\n",
    "prompt = \"Please, tell me some funny jokes\"\n",
    "print(f\"\\nSending prompt to LLM: '{prompt}'\")\n",
    "\n",
    "# Invoke the LLM with the prompt\n",
    "# The invoke method is the standard way to call Runnables (like LLMs)\n",
    "response = chatgpt.invoke(prompt)\n",
    "\n",
    "# Print the response\n",
    "print(\"\\nLLM Response:\")\n",
    "print(response.content) # Access the content attribute for the string response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7a5f0f",
   "metadata": {},
   "source": [
    "### **Chat Messages**\n",
    "LangChain allows us to segmentate prompts into three main types.(System, Human, AI)\n",
    "\n",
    "* **System** - Helpful background context that tell the AI its high-level behavior.\n",
    "* **Human** - Messages that represent the user input. \n",
    "* **AI** - Messages that show the response of the AI model, they work as examples to the model. \n",
    "\n",
    "\n",
    "For more, see OpenAI's [documentation](https://platform.openai.com/docs/guides/chat/introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "279c5eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ChatOpenAI model for message interaction...\n",
      "ChatOpenAI model initialized.\n",
      "\n",
      "Sending messages to LLM:\n",
      "- System: \n",
      "You are an AI bot that help people decide where to travel.\n",
      "Always recommend three destination with a short sentence for each.\n",
      "\n",
      "- Ai: Hello! I am a traveller assistant, how can I help you?\n",
      "- Human: Where should I travel next?\n",
      "\n",
      "LLM Response:\n",
      "Here are three destination recommendations for you:\n",
      "\n",
      "1. **Kyoto, Japan**: Immerse yourself in the rich history and culture of Kyoto by exploring its ancient temples, traditional tea houses, and beautiful gardens.\n",
      "   \n",
      "2. **Barcelona, Spain**: Experience the vibrant energy of Barcelona with its stunning architecture, delicious cuisine, and lively street performances.\n",
      "   \n",
      "3. **Banff National Park, Canada**: Surround yourself with the breathtaking beauty of the Canadian Rockies in Banff National Park, where you can hike, ski, and relax in natural hot springs.\n"
     ]
    }
   ],
   "source": [
    "#  Basic Chat Model Interaction\n",
    "\n",
    "# LLM (import already in Cell 1)\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# Message types (imports already in Cell 1)\n",
    "# from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Ensure OPENAI_API_KEY is loaded from Cell 2\n",
    "\n",
    "# Initialize the Chat LLM\n",
    "print(\"Initializing ChatOpenAI model for message interaction...\")\n",
    "# The model name is already set to \"gpt-3.5-turbo\" in the original code\n",
    "chatgpt = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\", # Specify the model name\n",
    "    temperature=0,         # Temperature controls randomness\n",
    "    openai_api_key=OPENAI_API_KEY # Pass the API key directly\n",
    ")\n",
    "print(\"ChatOpenAI model initialized.\")\n",
    "\n",
    "# Define the desired high-level behavior using a SystemMessage\n",
    "high_level_behavior = \"\"\"\n",
    "You are an AI bot that help people decide where to travel.\n",
    "Always recommend three destination with a short sentence for each.\n",
    "\"\"\"\n",
    "\n",
    "# Define the conversation history as a list of message objects\n",
    "# Use the imported message classes (SystemMessage, AIMessage, HumanMessage)\n",
    "messages = [\n",
    "    SystemMessage(content=high_level_behavior),\n",
    "    AIMessage(content=\"Hello! I am a traveller assistant, how can I help you?\"),\n",
    "    HumanMessage(content=\"Where should I travel next?\"),\n",
    "]\n",
    "\n",
    "print(\"\\nSending messages to LLM:\")\n",
    "for msg in messages:\n",
    "    print(f\"- {msg.type.capitalize()}: {msg.content}\")\n",
    "\n",
    "# Invoke the LLM with the list of messages\n",
    "# The invoke method is the standard way to call Chat Models\n",
    "response = chatgpt.invoke(messages)\n",
    "\n",
    "# The response is a BaseMessage object (an AIMessage in this case)\n",
    "# Access the content attribute for the string response\n",
    "print(\"\\nLLM Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dc3bc5",
   "metadata": {},
   "source": [
    "You can also pass more chat history with responses from the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93fa5ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sending extended messages to LLM:\n",
      "- System: \n",
      "You are an AI bot that help people decide where to travel.\n",
      "Always recommend three destination with a short sentence for each.\n",
      "\n",
      "- Ai: Hello! I am a traveller assistant, how can I help you?\n",
      "- Human: Where should I travel next?\n",
      "- Ai: For your next travel destination, consider:\n",
      "1. Kyoto, Japan: Immerse yourself in ancient temples and serene gardens.\n",
      "2. Florence, Italy: Explore Renaissance art and historic architecture.\n",
      "3. Banff, Canada: Experience breathtaking mountain landscapes and outdoor activities.\n",
      "- Human: I love going to Museums?\n",
      "\n",
      "LLM Response:\n",
      "If you love museums, you might enjoy visiting:\n",
      "1. Paris, France: Home to world-class museums like the Louvre and Musée d'Orsay.\n",
      "2. New York City, USA: Explore the Metropolitan Museum of Art and the Museum of Modern Art.\n",
      "3. St. Petersburg, Russia: Discover the State Hermitage Museum and the Fabergé Museum.\n"
     ]
    }
   ],
   "source": [
    "# Continuing Basic Chat Interaction\n",
    "\n",
    "# Define the desired high-level behavior using a SystemMessage (re-defined for clarity in this cell)\n",
    "high_level_behavior = \"\"\"\n",
    "You are an AI bot that help people decide where to travel.\n",
    "Always recommend three destination with a short sentence for each.\n",
    "\"\"\"\n",
    "\n",
    "# Define the extended conversation history as a list of message objects\n",
    "# This list includes the previous turns and the new interaction\n",
    "messages = [\n",
    "    SystemMessage(content=high_level_behavior),\n",
    "    AIMessage(content=\"Hello! I am a traveller assistant, how can I help you?\"),\n",
    "    HumanMessage(content=\"Where should I travel next?\"),\n",
    "    AIMessage(content=\"For your next travel destination, consider:\\n1. Kyoto, Japan: Immerse yourself in ancient temples and serene gardens.\\n2. Florence, Italy: Explore Renaissance art and historic architecture.\\n3. Banff, Canada: Experience breathtaking mountain landscapes and outdoor activities.\"), # <--- Added AI response from previous turn\n",
    "    HumanMessage(content=\"I love going to Museums?\"), # <--- Added the new Human message\n",
    "]\n",
    "\n",
    "print(\"\\nSending extended messages to LLM:\")\n",
    "for msg in messages:\n",
    "    print(f\"- {msg.type.capitalize()}: {msg.content}\")\n",
    "\n",
    "\n",
    "# Invoke the LLM with the extended list of messages\n",
    "# The invoke method is the standard way to call Chat Models\n",
    "response = chatgpt.invoke(messages)\n",
    "\n",
    "# The response is a BaseMessage object (an AIMessage in this case)\n",
    "# Access the content attribute for the string response\n",
    "print(\"\\nLLM Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf20b6c",
   "metadata": {},
   "source": [
    "### **Text Embedding Model**\n",
    "\n",
    "When documents or string-variables are too long, things got quite complicated. \n",
    "\n",
    "**In order to be able to process them, we can embed and convert string variables into vectors** (a series of numbers that hold the semantic 'meaning' of your text).\n",
    "\n",
    "*What are embeddings?*\n",
    "\n",
    "To put it simple, a number representation of your text. This list of numbers contains the information about the meaning of the text, so we can find similar text with similar meaning by seeing their number representation. \n",
    "\n",
    "Mainly used when comparing different pieces of text or when dealing with huge texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb6e401",
   "metadata": {},
   "source": [
    "**TASK:**\n",
    "- First import the `Embeddings` model from langcgain.embeddings.\n",
    "- Define a text to embed. \n",
    "- Embed the text with the `.embed_query` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e14d0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OpenAI Embeddings model for testing...\n",
      "Embeddings model initialized.\n",
      "\n",
      "Text to embed: \"Hi! It's time to go to a Museum!\"\n",
      "Your embedding is length 1536\n",
      "Here's a sample of the vector (first 5 elements): [-0.0022889249958097935, -0.003987872041761875, -0.001093194354325533, -0.01741538941860199, -0.021277064457535744]...\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 (Actual): Testing OpenAI Embeddings\n",
    "\n",
    "# Import necessary components for this cell (already in Cell 1)\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Ensure OPENAI_API_KEY is loaded from Cell 2\n",
    "\n",
    "# 1. Initialize an instance of the OpenAI Embeddings model\n",
    "print(\"Initializing OpenAI Embeddings model for testing...\")\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\", # Specify the model name (standard and recommended)\n",
    "    openai_api_key=OPENAI_API_KEY    # Pass the API key directly\n",
    ")\n",
    "print(\"Embeddings model initialized.\")\n",
    "\n",
    "# 2. Define a text to embed\n",
    "text = \"Hi! It's time to go to a Museum!\"\n",
    "print(f\"\\nText to embed: \\\"{text}\\\"\")\n",
    "\n",
    "# 3. Embed the text using the model\n",
    "# Use the embed_query method for embedding a single piece of text intended for querying\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "\n",
    "# 4. Print information about the embedding\n",
    "print(f\"Your embedding is length {len(text_embedding)}\")\n",
    "print(f\"Here's a sample of the vector (first 5 elements): {text_embedding[:5]}...\")\n",
    "# The length should be 1536 for the text-embedding-ada-002 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fe0f3c",
   "metadata": {},
   "source": [
    "### **Chains**\n",
    "\n",
    "Conversation chains in the context of Langchain are a concept involving the sequential linking of multiple conversational elements to build complex interactions. The idea is to streamline and enhance the conversation flow.\n",
    "\n",
    "The most basic chain is the `ConversationChain`. However, we will use the `load_qa_chain` to query questions about our documents, as its main function is optimized for this task. \n",
    "\n",
    "You can go check all available chains in the [LangChain Documentation.](https://python.langchain.com/docs/modules/chains/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b34748fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Basic Conversation Chain...\n",
      "\n",
      "Running conversation.invoke('Hello!')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nainesh Rathod\\AppData\\Local\\Temp\\ipykernel_26868\\2629095289.py:17: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(llm=chatgpt)\n",
      "d:\\data\\document_chatbot\\venve\\Lib\\site-packages\\pydantic\\main.py:253: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Response:\n",
      "Hello! How are you today?\n"
     ]
    }
   ],
   "source": [
    "# Basic Conversation Chain\n",
    "\n",
    "# Import necessary components for this cell (already in Cell 1)\n",
    "# from langchain.chains import ConversationChain # Original import path\n",
    "\n",
    "# Ensure `llm` (ChatOpenAI) is initialized from a previous cell (e.g., Cell 3)\n",
    "\n",
    "print(\"Initializing Basic Conversation Chain...\")\n",
    "\n",
    "# Initialize the ConversationChain\n",
    "# Pass your Chat LLM instance to the chain\n",
    "# Note: ConversationChain is an older chain type. For RAG with conversation history,\n",
    "# the LCEL approach using create_history_aware_retriever is recommended.\n",
    "# However, we'll update this cell to use the modern import path for ConversationChain\n",
    "from langchain.chains.conversation.base import ConversationChain # Modern import path for ConversationChain\n",
    "\n",
    "conversation = ConversationChain(llm=chatgpt)\n",
    "\n",
    "# Run a turn in the conversation\n",
    "# Use the invoke method, passing the user input directly\n",
    "print(\"\\nRunning conversation.invoke('Hello!')\")\n",
    "response = conversation.invoke(\"Hello!\")\n",
    "\n",
    "# The response is a dictionary, often containing 'response' or 'output' key\n",
    "# Check the output structure if needed\n",
    "# print(response)\n",
    "\n",
    "print(\"\\nAI Response:\")\n",
    "# Access the response text - check the dictionary keys based on the chain output\n",
    "# For ConversationChain.invoke, the output key is typically 'response'\n",
    "print(response.get('response'))\n",
    "\n",
    "# You can run another turn by invoking the chain again\n",
    "# print(\"\\nRunning conversation.invoke('How are you today?')\")\n",
    "# response2 = conversation.invoke(\"How are you today?\")\n",
    "# print(\"\\nAI Response:\")\n",
    "# print(response2.get('response'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a8ce2e",
   "metadata": {},
   "source": [
    "### Memory\n",
    "When interacting with a model, it is important to keep track of all interactions performed with it. \n",
    "\n",
    "To overcome these limitations, langchain implements different types of memories to use in your application.\n",
    "\n",
    "It is important to consider that storing all the interactions with the model can quickly escalate to a considerable amount of tokens to process every time we prompt the model. It is essential to bear in mind that ChatGPT has a token limit per interaction.\n",
    "\n",
    "You can learn more about memory [here]([https://towardsdatascience.com/custom-memory-for-chatgpt-api-artificial-intelligence-python-722d627d4d6d])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71b182be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ConversationSummaryBufferMemory...\n",
      "Memory initialized.\n",
      "\n",
      "Manually saving context to memory...\n",
      "Context saved.\n",
      "\n",
      "(Re)initializing ChatOpenAI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nainesh Rathod\\AppData\\Local\\Temp\\ipykernel_26868\\2795672115.py:21: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI model initialized.\n",
      "\n",
      "Initializing ConversationChain with Memory...\n",
      "ConversationChain initialized.\n",
      "\n",
      "Running conversation.invoke('What cities do you recommend me?')\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Can you recommend me where should I travel next?', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello! I am a traveller assistant, sure I can help you. What do you enjoy doing?', additional_kwargs={}, response_metadata={}), HumanMessage(content='I love going to Museums', additional_kwargs={}, response_metadata={}), AIMessage(content='Great then you should go to a cultural capital.', additional_kwargs={}, response_metadata={})]\n",
      "Human: What cities do you recommend me?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "AI Response:\n",
      "I recommend cities like Paris, London, Rome, and New York City. These cities have a rich cultural history and are home to some of the world's most famous museums. Each city offers a unique experience for museum lovers with a wide range of art, history, and science museums to explore. Let me know if you would like more information on any specific city!\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 (Actual): Demonstrating Conversation Memory and Basic Conversation Chain with Memory\n",
    "\n",
    "# Import necessary components for this cell (imports already in Cell 1)\n",
    "# from langchain.memory import ConversationSummaryBufferMemory # Original import path\n",
    "# from langchain.chains import ConversationChain # Original import path\n",
    "# from langchain_openai import ChatOpenAI # LLM import\n",
    "\n",
    "# Ensure OPENAI_API_KEY is loaded from Cell 2\n",
    "# Ensure `llm` (ChatOpenAI) is initialized from a previous cell (e.g., Cell 3 or 4)\n",
    "\n",
    "# Modern import path for ConversationSummaryBufferMemory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# Modern import path for ConversationChain (used in Cell 6)\n",
    "from langchain.chains.conversation.base import ConversationChain\n",
    "\n",
    "# 1. Initialize Conversation Summary Buffer Memory\n",
    "\n",
    "print(\"Initializing ConversationSummaryBufferMemory...\")\n",
    "# Pass your Chat LLM instance for summarization and set the max token limit\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chatgpt, # The LLM used for summarizing older messages\n",
    "    max_token_limit=100, # Max number of tokens to store in the buffer before summarizing\n",
    "    memory_key=\"history\", # Key for the history variable (can be anything, but \"history\" is common)\n",
    "    return_messages=True # Return history as a list of message objects\n",
    ")\n",
    "print(\"Memory initialized.\")\n",
    "\n",
    "# 2. Manually Save Context to Memory\n",
    "\n",
    "print(\"\\nManually saving context to memory...\")\n",
    "# Use the .save_context() method to add a turn (input/output pair) to memory\n",
    "# The method expects dictionaries for the input and output of a conversational turn\n",
    "memory.save_context(\n",
    "    {\"input\":  \"Can you recommend me where should I travel next?\"},\n",
    "    {\"output\": \"Hello! I am a traveller assistant, sure I can help you. What do you enjoy doing?\"}\n",
    ")\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\":  \"I love going to Museums\"},\n",
    "    {\"output\": \"Great then you should go to a cultural capital.\"}\n",
    ")\n",
    "print(\"Context saved.\")\n",
    "\n",
    "# You can check the current state of the memory (optional)\n",
    "# print(\"\\nCurrent state of memory:\")\n",
    "# print(memory.load_memory_variables({}))\n",
    "\n",
    "# 3. (Re)initialize ChatOpenAI (assuming you might want to re-initialize here per original notebook)\n",
    "# If llm was already initialized in a prior cell, this might be redundant,\n",
    "# but following the notebook's sequence:\n",
    "# Note: Ensure OPENAI_API_KEY is defined (from Cell 2)\n",
    "print(\"\\n(Re)initializing ChatOpenAI model...\")\n",
    "chatgpt_for_chain = ChatOpenAI( # Use a different variable name if llm is still in use\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_key=OPENAI_API_KEY # Pass API key\n",
    ")\n",
    "print(\"ChatOpenAI model initialized.\")\n",
    "\n",
    "\n",
    "# 4. Initialize ConversationChain with Memory\n",
    "\n",
    "print(\"\\nInitializing ConversationChain with Memory...\")\n",
    "conversation = ConversationChain(\n",
    "    llm=chatgpt_for_chain, # Use the LLM instance\n",
    "    memory=memory,         # Pass the initialized memory object\n",
    "    verbose=True           # Set verbose=True to see the prompts being sent to the LLM\n",
    ")\n",
    "print(\"ConversationChain initialized.\")\n",
    "\n",
    "# 5. Run a Conversational Turn using the Chain\n",
    "\n",
    "user_input = \"What cities do you recommend me?\"\n",
    "print(f\"\\nRunning conversation.invoke('{user_input}')\")\n",
    "\n",
    "# Use the .invoke() method to run a turn in the conversation chain\n",
    "# Pass the user input as a string (ConversationChain is designed for simple string input/output)\n",
    "response = conversation.invoke(user_input)\n",
    "\n",
    "# The response from ConversationChain.invoke is a dictionary\n",
    "# The final AI response is typically under the key 'response'\n",
    "print(\"\\nAI Response:\")\n",
    "print(response.get('response')) # Use .get() for safe access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3e7b67",
   "metadata": {},
   "source": [
    "### Dealing with Documents\n",
    "\n",
    "We are here to deal with documents... so LangChain provides a wide variety of elements to deal with them. \n",
    "\n",
    "One of the most important improvements of LangChain is that it allows us to upload documents and pass them to our model. \n",
    "We consider a document as an object that holds a piece of text and metadata (more information about that text)\n",
    "\n",
    "- Document class\n",
    "- Document Loader\n",
    "- Document Retriever\n",
    "- Text Splitter\n",
    "- Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cee5a0",
   "metadata": {},
   "source": [
    "**TASK**\n",
    "\n",
    "1. From langchain.schema import the `Document` class. \n",
    "2. Now define a document that has \n",
    "   - Text contained in page_content. \n",
    "   - Metada composed of document_id, document_source and document_create_time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b0f4859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstrating the Document object structure.\n",
      "\n",
      "Created Document object:\n",
      "page_content='Let's imagine this is a huge document with a lot of words and important stuff.' metadata={'document_id': '0000', 'document_source': 'my_source.pdf', 'document_create_time': '01/01/2000'}\n",
      "\n",
      "Page Content: Let's imagine this is a huge document with a lot of words and important stuff.\n",
      "Metadata: {'document_id': '0000', 'document_source': 'my_source.pdf', 'document_create_time': '01/01/2000'}\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 (Actual): Demonstrating the Document Object\n",
    "\n",
    "# Import the Document class (import already in Cell 1)\n",
    "# from langchain.schema import Document # Original import path\n",
    "\n",
    "# Modern import path for the Document class\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "print(\"Demonstrating the Document object structure.\")\n",
    "\n",
    "# 1. Define the content and metadata for the document\n",
    "page_content = \"Let's imagine this is a huge document with a lot of words and important stuff.\"\n",
    "metadata = {\n",
    "    'document_id' : '0000', # Changed to string as IDs often are\n",
    "    'document_source' : \"my_source.pdf\",\n",
    "    'document_create_time' : \"01/01/2000\"\n",
    "}\n",
    "\n",
    "# 2. Create an instance of the Document object\n",
    "document = Document(page_content=page_content, metadata=metadata)\n",
    "\n",
    "# Print the document object\n",
    "print(\"\\nCreated Document object:\")\n",
    "print(document)\n",
    "\n",
    "# You can access its attributes\n",
    "print(f\"\\nPage Content: {document.page_content}\")\n",
    "print(f\"Metadata: {document.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7afa87",
   "metadata": {},
   "source": [
    "#### Document Loaders\n",
    "\n",
    "Depending on where our data is stored, we will need a different type of loader:\n",
    "\n",
    "- The **Online Loader** is used for loading a document directly from the Internet. LangChain implements different types of loaders. For example, there is the `WikipediaLoader` that helps you loading Wikipedia pages or the `HNLoader` to take content directly from any HackerNews page.\n",
    "\n",
    "\n",
    "\n",
    "- The **Offline Loader** is used loading a document stored that are already installed in your machine. There are also different types of offline loaders such as the **HTML** loader for `.html` pages or the **PyPDFLoader** for `.pdf` documents.\n",
    "\n",
    "In this tutorial, we will see an example of Online Loader by using the `WikipediaLoader` and the `HNLoader`, and an example of Offline Loader by using the PyPDFLoader.\n",
    "\n",
    "You can find a list of the supported [LangChain Document Loaders](https://python.langchain.com/docs/integrations/document_loaders) in the official documentation. Those Loaders are from external integrations, [native LangChain Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/) can be found in the official documentation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f8122f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstrating loading data from Wikipedia.\n",
      "Loading content for topic: 'Machine_learning' from Wikipedia...\n",
      "Successfully loaded 25 documents from Wikipedia.\n",
      "\n",
      "Content of the first Wikipedia document:\n",
      "Source: https://en.wikipedia.org/wiki/Machine_learning\n",
      "Title: Machine learning\n",
      "Summary: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\n",
      "ML finds application in many fields, ...\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 (Actual): Loading Data with WikipediaLoader\n",
    "\n",
    "# Import necessary components for this cell (import in Cell 1 was for PyPDFLoader)\n",
    "# The WikipediaLoader is part of the langchain-community package in modern versions\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "print(\"Demonstrating loading data from Wikipedia.\")\n",
    "\n",
    "# Define the topic to load from Wikipedia\n",
    "topic = \"Machine_learning\"\n",
    "print(f\"Loading content for topic: '{topic}' from Wikipedia...\")\n",
    "\n",
    "# Initialize the WikipediaLoader\n",
    "loader = WikipediaLoader(topic)\n",
    "\n",
    "# Load content from Wikipedia. This returns a list of Document objects.\n",
    "# Each Document might represent a section or chunk of the Wikipedia page.\n",
    "try:\n",
    "    wikipedia_data = loader.load()\n",
    "    print(f\"Successfully loaded {len(wikipedia_data)} documents from Wikipedia.\")\n",
    "\n",
    "    # Display the first document loaded\n",
    "    if wikipedia_data:\n",
    "        print(\"\\nContent of the first Wikipedia document:\")\n",
    "        # print(wikipedia_data[0]) # Print the whole Document object\n",
    "        print(f\"Source: {wikipedia_data[0].metadata.get('source')}\")\n",
    "        print(f\"Title: {wikipedia_data[0].metadata.get('title')}\")\n",
    "        print(f\"Summary: {wikipedia_data[0].page_content[:500]}...\") # Print first 500 chars\n",
    "    else:\n",
    "        print(\"No data was loaded from Wikipedia.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from Wikipedia: {e}\")\n",
    "    print(\"Please ensure you have the 'wikipedia' library installed (`pip install wikipedia`)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1c6c70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting the first loaded Wikipedia document.\n",
      "\n",
      "Page Content:\n",
      "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\n",
      "ML finds application in many fields, ...\n",
      "\n",
      "Meta Data:\n",
      "{'title': 'Machine learning', 'summary': 'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\\nML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\\nStatistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. \\nFrom a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning.', 'source': 'https://en.wikipedia.org/wiki/Machine_learning'}\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 (Actual): Inspecting a Loaded Document\n",
    "\n",
    "# Ensure wikipedia_data is loaded from the previous cell (Cell 9)\n",
    "\n",
    "print(\"Inspecting the first loaded Wikipedia document.\")\n",
    "\n",
    "# Access and print the page_content attribute\n",
    "print(\"\\nPage Content:\")\n",
    "if wikipedia_data and wikipedia_data[0].page_content:\n",
    "    # Print the first 500 characters for brevity if it's long\n",
    "    print(wikipedia_data[0].page_content[:500] + \"...\")\n",
    "else:\n",
    "    print(\"Page content not found or wikipedia_data is empty.\")\n",
    "\n",
    "\n",
    "# Access and print the metadata attribute\n",
    "print(\"\\nMeta Data:\")\n",
    "if wikipedia_data and wikipedia_data[0].metadata:\n",
    "     print(wikipedia_data[0].metadata)\n",
    "else:\n",
    "    print(\"Metadata not found or wikipedia_data is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbcd3073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstrating loading data with HNLoader and PyPDFLoader/PyPDFDirectoryLoader.\n",
      "\n",
      "--- Loading from Hacker News ---\n",
      "Loading content from URL: https://news.ycombinator.com/item?id=34422627\n",
      "Successfully loaded 76 documents from Hacker News.\n",
      "First HN document snippet:\n",
      "Source: https://news.ycombinator.com/item?id=34422627\n",
      "Author: None\n",
      "Content: Ozzie_osman on Jan 18, 2023  \n",
      "             | next [–] \n",
      "\n",
      "LangChain is awesome. For people not sure what it's doing, large language models (LLMs) are very powerful but they're very general. As a common example for this limitation, imagine you want your LLM to answer questions over a large corpus.You can't pass the entire corpus into the prompt. So you might:\n",
      "- preprocess the corpus by iterating over documents, splitting them into chunks, and summarizing them\n",
      "- embed those chunks/summaries in some ...\n",
      "\n",
      "--- Loading a single local PDF ---\n",
      "Loading content from local PDF: Docs/attentions.pdf\n",
      "Successfully loaded 15 documents (pages) from Docs/attentions.pdf.\n",
      "First PDF document snippet:\n",
      "Source: Docs/attentions.pdf, Page: 0\n",
      "Content: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz ...\n",
      "\n",
      "--- Loading all PDFs from a directory ---\n",
      "Loading content from directory: Docs/\n"
     ]
    }
   ],
   "source": [
    "# Cell 11 (Actual): Demonstrating Other Document Loaders\n",
    "\n",
    "# Import necessary loaders using their modern paths (part of langchain-community)\n",
    "from langchain_community.document_loaders import HNLoader, PyPDFLoader, PyPDFDirectoryLoader\n",
    "\n",
    "print(\"Demonstrating loading data with HNLoader and PyPDFLoader/PyPDFDirectoryLoader.\")\n",
    "\n",
    "# --- 1. HNLoader (Loading from a Hacker News URL) ---\n",
    "print(\"\\n--- Loading from Hacker News ---\")\n",
    "hn_url = \"https://news.ycombinator.com/item?id=34422627\"\n",
    "print(f\"Loading content from URL: {hn_url}\")\n",
    "\n",
    "try:\n",
    "    # Initialize and load with HNLoader\n",
    "    hn_loader = HNLoader(hn_url)\n",
    "    hn_data = hn_loader.load()\n",
    "\n",
    "    print(f\"Successfully loaded {len(hn_data)} documents from Hacker News.\")\n",
    "\n",
    "    # Inspect the loaded data (optional)\n",
    "    if hn_data:\n",
    "        print(\"First HN document snippet:\")\n",
    "        # HNLoader often puts the comment content in page_content and metadata has author, url etc.\n",
    "        print(f\"Source: {hn_data[0].metadata.get('source')}\")\n",
    "        print(f\"Author: {hn_data[0].metadata.get('author')}\")\n",
    "        print(f\"Content: {hn_data[0].page_content[:500]}...\") # Print first 500 chars\n",
    "    else:\n",
    "        print(\"No data was loaded from the Hacker News URL.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from Hacker News: {e}\")\n",
    "    # HNLoader might have dependencies or connection issues\n",
    "\n",
    "\n",
    "# --- 2. PyPDFLoader (Loading a single local PDF file) ---\n",
    "print(\"\\n--- Loading a single local PDF ---\")\n",
    "# <--- IMPORTANT: Change this path to YOUR local PDF file ---\n",
    "single_pdf_path = \"Docs/attentions.pdf\" # Example path from original code\n",
    "print(f\"Loading content from local PDF: {single_pdf_path}\")\n",
    "\n",
    "try:\n",
    "    # Initialize and load with PyPDFLoader\n",
    "    pdf_loader = PyPDFLoader(single_pdf_path)\n",
    "    pdf_data = pdf_loader.load()\n",
    "\n",
    "    print(f\"Successfully loaded {len(pdf_data)} documents (pages) from {single_pdf_path}.\")\n",
    "\n",
    "    # Inspect the loaded data (optional)\n",
    "    if pdf_data:\n",
    "        print(\"First PDF document snippet:\")\n",
    "        print(f\"Source: {pdf_data[0].metadata.get('source')}, Page: {pdf_data[0].metadata.get('page')}\")\n",
    "        print(f\"Content: {pdf_data[0].page_content[:500]}...\") # Print first 500 chars\n",
    "    else:\n",
    "         print(\"No data was loaded from the local PDF.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{single_pdf_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading PDF file: {e}\")\n",
    "\n",
    "\n",
    "# --- 3. PyPDFDirectoryLoader (Loading all PDFs from a directory) ---\n",
    "print(\"\\n--- Loading all PDFs from a directory ---\")\n",
    "# <--- IMPORTANT: Change this path to YOUR local directory containing PDFs ---\n",
    "pdf_directory_path = \"Docs/\" # Example path from original code\n",
    "print(f\"Loading content from directory: {pdf_directory_path}\")\n",
    "\n",
    "# Uncomment the following lines to run the directory loader example\n",
    "# try:\n",
    "#     # Initialize and load with PyPDFDirectoryLoader\n",
    "#     directory_loader = PyPDFDirectoryLoader(pdf_directory_path)\n",
    "#     pdf_directory_data = directory_loader.load()\n",
    "\n",
    "#     print(f\"Successfully loaded {len(pdf_directory_data)} documents (pages) from directory {pdf_directory_path}.\")\n",
    "\n",
    "#     # Inspect the loaded data (optional)\n",
    "#     if pdf_directory_data:\n",
    "#         print(\"First document snippet from directory loading:\")\n",
    "#         print(f\"Source: {pdf_directory_data[0].metadata.get('source')}, Page: {pdf_directory_data[0].metadata.get('page')}\")\n",
    "#         print(f\"Content: {pdf_directory_data[0].page_content[:500]}...\") # Print first 500 chars\n",
    "#     else:\n",
    "#          print(\"No data was loaded from the directory.\")\n",
    "\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Error: The directory '{pdf_directory_path}' was not found.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading PDFs from directory: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b54820",
   "metadata": {},
   "source": [
    "#### Text Splitter\n",
    "\n",
    "**Data Chunks and Model Tokenizer**\n",
    "\n",
    "To efficiently handle data when building an LLM-based application, data needs to be divided in portions. Those are the so-called data chunks and the chunk size is highly determinant in the quality of the chatbot.\n",
    "\n",
    "The tokenizer plays a crucial role in relation to data chunks when working with LLMs: \n",
    "- A **tokenizer is the tool used to convert text data into a format that can be processed by the model.**\n",
    "- Data is then stored in the vector stores in the tokenized format.\n",
    "\n",
    "To convert the original data into tokens and split it in data chunks, we will use the **LangChain Text Splitter**.\n",
    "\n",
    "If you are interested in more details about the tokenizer, the article [Unleashing the ChatGPT Tokenizer](https://medium.com/towards-data-science/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54) is for you!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd6d7b",
   "metadata": {},
   "source": [
    "By using Langchain, we can highly customize how to split our data:\n",
    "- **Split by chunks**: The most general approach is to split your data into chunks of a concrete size. In the following example, we will take the data that we have already loaded (`wikipedia_data`, `hn_data` and `pdf_data`) and we will split it in portions of 200 characters. \n",
    "\n",
    "_What will happen if the split based on character count breaks a word?_\n",
    "\n",
    "There is the concept of \"chunk overlap\" that refers to a method where consecutive chunks of text share some common content. This technique is used to maintain context and coherence when a long document is divided into smaller parts due to the token limitations of LLMs. In this case, we will use a chunk size of 20 characters.\n",
    "\n",
    "So let's split the Wikipedia data we have just loaded: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49e08ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstrating token-based text splitting.\n",
      "tiktoken tokenizer initialized.\n",
      "\n",
      "Initializing RecursiveCharacterTextSplitter...\n",
      "\n",
      "SPLITTING DOCUMENTS INTO CHUNKS\n",
      "Splitting 25 Wikipedia documents...\n",
      "Wikipedia Data - Now you have 138 number of chunks.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12 (Actual): Splitting Documents into Chunks (Token-based)\n",
    "\n",
    "# Import necessary components for this cell (imports already in Cell 1)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from transformers import GPT2TokenizerFast # Original import - we'll use tiktoken instead\n",
    "import tiktoken # Recommended tokenizer for OpenAI models\n",
    "\n",
    "# Ensure `wikipedia_data` is loaded from a previous cell (Cell 9)\n",
    "\n",
    "print(\"Demonstrating token-based text splitting.\")\n",
    "\n",
    "# Use tiktoken for token counting with OpenAI models\n",
    "# Get the tokenizer encoding for the embedding model you plan to use (e.g., text-embedding-ada-002)\n",
    "# 'cl100k_base' is common for modern OpenAI models\n",
    "try:\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    print(\"tiktoken tokenizer initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing tiktoken tokenizer: {e}\")\n",
    "    print(\"Please ensure you have the 'tiktoken' library installed (`pip install tiktoken`)\")\n",
    "    tokenizer = None # Set to None if initialization fails\n",
    "\n",
    "# Create function to count tokens using tiktoken\n",
    "def count_tokens_tiktoken(text: str) -> int:\n",
    "    if tokenizer:\n",
    "        tokens = tokenizer.encode(text, disallowed_special=())\n",
    "        return len(tokens)\n",
    "    return len(text) # Fallback to character count if tiktoken failed\n",
    "\n",
    "\n",
    "# Define the chunk size and overlap in tokens\n",
    "chunk_size = 200 # Example size from original code\n",
    "chunk_overlap = 20 # Example overlap from original code\n",
    "\n",
    "# Initialize the text splitter\n",
    "print(\"\\nInitializing RecursiveCharacterTextSplitter...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=count_tokens_tiktoken if tokenizer else len, # Use tiktoken function if available, else len\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"] # Common separators\n",
    ")\n",
    "\n",
    "print(\"\\nSPLITTING DOCUMENTS INTO CHUNKS\")\n",
    "\n",
    "# Split the loaded Wikipedia documents into chunks\n",
    "if wikipedia_data:\n",
    "    print(f\"Splitting {len(wikipedia_data)} Wikipedia documents...\")\n",
    "    wikipedia_chunks = text_splitter.split_documents(wikipedia_data)\n",
    "    print(f\"Wikipedia Data - Now you have {len(wikipedia_chunks)} number of chunks.\")\n",
    "\n",
    "    # Inspect a chunk (optional)\n",
    "    # if wikipedia_chunks:\n",
    "    #     print(\"\\nFirst Wikipedia chunk snippet:\")\n",
    "    #     print(wikipedia_chunks[0].page_content[:500] + \"...\")\n",
    "    #     print(f\"Metadata: {wikipedia_chunks[0].metadata}\")\n",
    "    #     print(f\"Token count (tiktoken): {count_tokens_tiktoken(wikipedia_chunks[0].page_content)}\")\n",
    "else:\n",
    "    print(\"No Wikipedia data loaded to split.\")\n",
    "\n",
    "# Note: You would perform a similar splitting step for your PDF documents (using the 'texts' variable from Cell 3)\n",
    "# pdf_chunks = text_splitter.split_documents(texts)\n",
    "# print(f\"PDF Data - Now you have {len(pdf_chunks)} number of chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d52c3",
   "metadata": {},
   "source": [
    "**TASK:**\n",
    "\n",
    "Generate the chunks for both `HNLoader` and `PyPDFLoader`. \n",
    "1. Import both RecursiveCharacterTextSplitter from langchain.text_splitter and GPT2TokenizerFast from transformers. \n",
    "2. Define a tokenizer with the following command: tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "3. Define a count_tokens function that will allow us to count the tokens of out text. \n",
    "4. Define the text_splitter with a chunk_size of 200, a chunk_overlap of 20 and the length_function we have just defined. \n",
    "5. Apply the command `.split_documents`to our data. \n",
    "\n",
    "You can define your own function for the HN data and use your the default function for the PDF Data.  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "570894d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying text splitting to PDF and HN data.\n",
      "\n",
      "--- Splitting PDF Data ---\n",
      "tiktoken tokenizer initialized for PDF splitting.\n",
      "Splitting 15 PDF documents (pages)...\n",
      "PDF Data - Now you have 16 number of chunks.\n",
      "\n",
      "--- Splitting HN Data ---\n",
      "Splitting 76 HN documents...\n",
      "HN Data - Now you have 248 number of chunks.\n"
     ]
    }
   ],
   "source": [
    "# Cell 13 (Actual): Applying Text Splitting\n",
    "\n",
    "# Import necessary components (imports already in Cell 1)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from transformers import GPT2TokenizerFast # Original import - replaced by tiktoken recommendation\n",
    "import tiktoken # Recommended tokenizer for OpenAI models\n",
    "\n",
    "# Ensure `pdf_data` is loaded from a previous cell (e.g., Cell 3 or 11)\n",
    "# Ensure `hn_data` is loaded from a previous cell (e.g., Cell 11)\n",
    "\n",
    "print(\"Applying text splitting to PDF and HN data.\")\n",
    "\n",
    "# --- Text Splitting for PDF Data (Token-based) ---\n",
    "print(\"\\n--- Splitting PDF Data ---\")\n",
    "\n",
    "# Use tiktoken for token counting (recommended for OpenAI embeddings)\n",
    "# The tokenizer was likely initialized in Cell 12, but re-initializing for clarity in this block\n",
    "try:\n",
    "    # Get the tokenizer encoding for the embedding model (e.g., text-embedding-ada-002)\n",
    "    tokenizer_pdf = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    print(\"tiktoken tokenizer initialized for PDF splitting.\")\n",
    "    pdf_length_function = lambda text: len(tokenizer_pdf.encode(text, disallowed_special=()))\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing tiktoken tokenizer for PDF: {e}\")\n",
    "    print(\"Falling back to character count for PDF splitting.\")\n",
    "    pdf_length_function = len # Fallback to character count if tiktoken fails\n",
    "\n",
    "\n",
    "# Define the splitter for PDF data (using token count)\n",
    "# Use chunk_size and chunk_overlap appropriate for your embedding model (e.g., 1000/200 or 200/20 as in original)\n",
    "pdf_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, # Recommend slightly larger chunks for RAG context, adjust as needed\n",
    "    chunk_overlap=200, # Recommend overlap\n",
    "    length_function=pdf_length_function, # Use the token or fallback length function\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"] # Common separators\n",
    ")\n",
    "\n",
    "# Apply the splitter to the loaded PDF data\n",
    "if 'pdf_data' in locals() and pdf_data: # Check if pdf_data variable exists and is not empty\n",
    "    print(f\"Splitting {len(pdf_data)} PDF documents (pages)...\")\n",
    "    pdf_chunks = pdf_text_splitter.split_documents(pdf_data)\n",
    "    print(f\"PDF Data - Now you have {len(pdf_chunks)} number of chunks.\")\n",
    "    # Optional: Inspect a chunk\n",
    "    # if pdf_chunks:\n",
    "    #      print(f\"First PDF chunk token count: {count_tokens_tiktoken(pdf_chunks[0].page_content)}\")\n",
    "else:\n",
    "    print(\"pdf_data not found or is empty. Skipping PDF splitting.\")\n",
    "\n",
    "\n",
    "# --- Text Splitting for HN Data (Character-based example) ---\n",
    "print(\"\\n--- Splitting HN Data ---\")\n",
    "\n",
    "# Define the splitter for HN data (using default character count - len())\n",
    "# Original code used 200/20, let's keep that for this example\n",
    "hn_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    # length_function is len() by default, so no need to specify it if you want char count\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"] # Common separators\n",
    ")\n",
    "\n",
    "# Apply the splitter to the loaded HN data\n",
    "if 'hn_data' in locals() and hn_data: # Check if hn_data variable exists and is not empty\n",
    "    print(f\"Splitting {len(hn_data)} HN documents...\")\n",
    "    hn_chunks = hn_text_splitter.split_documents(hn_data)\n",
    "    print(f\"HN Data - Now you have {len(hn_chunks)} number of chunks.\")\n",
    "    # Optional: Inspect a chunk\n",
    "    # if hn_chunks:\n",
    "    #      print(f\"First HN chunk character count: {len(hn_chunks[0].page_content)}\")\n",
    "else:\n",
    "    print(\"hn_data not found or is empty. Skipping HN splitting.\")\n",
    "\n",
    "# Note: The 'pdf_chunks' variable is what you will use for embedding and upserting to Pinecone\n",
    "# for your RAG project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45013cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing token count distribution of PDF chunks.\n",
      "\n",
      "Generating histogram for 16 chunks...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 14 (Actual): Visualizing Chunk Token Counts\n",
    "\n",
    "# Import necessary libraries for data visualization (imports already in Cell 1)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure `pdf_chunks` is created from a previous cell (e.g., Cell 13 Actual)\n",
    "# Ensure the token counting function (`count_tokens_tiktoken`) is defined and the tokenizer is initialized\n",
    "# from a previous cell (e.g., Cell 12 Actual)\n",
    "\n",
    "print(\"Visualizing token count distribution of PDF chunks.\")\n",
    "\n",
    "# Create a list of token counts for each PDF chunk\n",
    "# Use the updated tiktoken counting function defined earlier (e.g., in Cell 12 Actual)\n",
    "if 'pdf_chunks' in locals() and pdf_chunks: # Check if pdf_chunks exists and is not empty\n",
    "    token_counts = [count_tokens_tiktoken(chunk.page_content) for chunk in pdf_chunks]\n",
    "\n",
    "    # Create a DataFrame from the token counts\n",
    "    df = pd.DataFrame({'Token Count': token_counts})\n",
    "\n",
    "    # Create a histogram of the token count distribution\n",
    "    print(f\"\\nGenerating histogram for {len(token_counts)} chunks...\")\n",
    "    df.hist(bins=40) # You can adjust the number of bins\n",
    "\n",
    "    # Set the title and labels for clarity\n",
    "    plt.title('Distribution of PDF Chunk Token Counts')\n",
    "    plt.xlabel('Token Count')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: Print descriptive statistics\n",
    "    # print(\"\\nToken count descriptive statistics:\")\n",
    "    # print(df.describe())\n",
    "\n",
    "else:\n",
    "    print(\"pdf_chunks not found or is empty. Skipping visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0fcfee",
   "metadata": {},
   "source": [
    "- **Split by pages**: If your data comes from documents organized in pages, there are methods that allow you to split data in pages to keep track of the page content. This method is specially useful when dealing with PDFs, as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb257ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SPLITTING BY PAGES\n",
      "PDF Splited by Pages - You have 16 number of chunks.\n"
     ]
    }
   ],
   "source": [
    "# Simple method - Split by pages    ________________________________________________________________________\n",
    "# You need a PDF file in your environement. \n",
    "loader = PyPDFLoader(\"Docs/attentions.pdf\")\n",
    "pdf_pages_chunks = loader.load_and_split()\n",
    "pdf_pages_chunks\n",
    "\n",
    "print(\"\\nSPLITTING BY PAGES\")\n",
    "print(\"PDF Splited by Pages - You have {0} number of chunks.\".format(len(pdf_pages_chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f35866",
   "metadata": {},
   "source": [
    "### Vector Stores\n",
    "\n",
    "Vector stores, also known as vector databases, are specialized types of databases designed to efficiently handle and manipulate high-dimensional vector data. In our case, we will store the tokenized and splitted content, e.g., the data chunks in the format that LLMs can process.\n",
    "\n",
    "There are different types of vector stores. Depending on the storage of the data, we can classify them as:\n",
    "- **Local Vector Stores**: This type of databases store the information in your local system. As an example of Local Vector Store, we will use FAISS.\n",
    "- **Online Vector Stores**: This type of databases store the information in the cloud. We will use Pinecone as out preferred option for Online Vector Stores.\n",
    "\n",
    "FAISS - EXAMPLE OF LOCAL VECTOR STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e78d52d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstrating creating a local FAISS vector store.\n",
      "Initializing OpenAI Embeddings model...\n",
      "Embeddings model initialized.\n",
      "\n",
      "Creating FAISS vector database from pdf_chunks...\n",
      "FAISS vector database created.\n"
     ]
    }
   ],
   "source": [
    "# Cell 15 (Actual): Creating a Local FAISS Vector Store\n",
    "\n",
    "# Import necessary vector store and embedding components (imports already in Cell 1)\n",
    "# from langchain.vectorstores import FAISS # Original import path\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings # Original import path\n",
    "\n",
    "# Modern import path for FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Modern import path for OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Ensure `pdf_chunks` is created from a previous cell (e.g., Cell 13 Actual)\n",
    "# Ensure `OPENAI_API_KEY` is loaded from environment variables (Cell 2)\n",
    "\n",
    "print(\"Demonstrating creating a local FAISS vector store.\")\n",
    "\n",
    "# 1. Get or Initialize the embedding model\n",
    "# Assuming embeddings instance might already exist from Cell 5, but initializing again for clarity\n",
    "# Ensure OPENAI_API_KEY is loaded.\n",
    "print(\"Initializing OpenAI Embeddings model...\")\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\", # Specify the model\n",
    "    openai_api_key=OPENAI_API_KEY    # Pass the API key\n",
    ")\n",
    "print(\"Embeddings model initialized.\")\n",
    "\n",
    "\n",
    "# 2. Create a vector database using FAISS from documents\n",
    "\n",
    "# Use the FAISS.from_documents class method.\n",
    "# This method takes your document chunks and the embedding model,\n",
    "# generates embeddings for the chunks, and builds the FAISS index.\n",
    "print(\"\\nCreating FAISS vector database from pdf_chunks...\")\n",
    "if 'pdf_chunks' in locals() and pdf_chunks: # Check if pdf_chunks exists and is not empty\n",
    "    db_FAISS = FAISS.from_documents(pdf_chunks, embeddings)\n",
    "    print(\"FAISS vector database created.\")\n",
    "\n",
    "    # Optional: You can save the FAISS index to disk if you want to reuse it later\n",
    "    # faiss_save_path = \"./my_faiss_index\"\n",
    "    # print(f\"Saving FAISS index to {faiss_save_path}...\")\n",
    "    # db_FAISS.save_local(faiss_save_path)\n",
    "    # print(\"FAISS index saved.\")\n",
    "\n",
    "    # Optional: Load a saved FAISS index\n",
    "    # print(f\"Loading FAISS index from {faiss_save_path}...\")\n",
    "    # loaded_db_FAISS = FAISS.load_local(faiss_save_path, embeddings, allow_dangerous_deserialization=True) # allow_dangerous_deserialization needed for recent versions\n",
    "    # print(\"FAISS index loaded.\")\n",
    "\n",
    "else:\n",
    "    print(\"pdf_chunks not found or is empty. Skipping FAISS database creation.\")\n",
    "\n",
    "\n",
    "# Note: This creates a local FAISS index (db_FAISS) for testing.\n",
    "# Your main project uses a Pinecone index (vectorstore) initialized in Cell 6.\n",
    "# Subsequent query steps might use either db_FAISS or vectorstore as the retriever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e11ee3",
   "metadata": {},
   "source": [
    "PINECONE - EXAMPLE OF ONLINE VECTOR STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2017afe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Pinecone client...\n",
      "Pinecone client initialized successfully.\n",
      "\n",
      "Checking for Pinecone index: 'langchain'\n",
      "Index 'langchain' already exists.\n",
      "\n",
      "Connected to index 'langchain'. Index stats:\n",
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "# Cell 16 (Actual): Initializing Pinecone Client and Creating/Connecting to Index (Fixed)\n",
    "\n",
    "# Import necessary Pinecone components (imports already in Cell 1)\n",
    "# from pinecone import Pinecone, ServerlessSpec # Or PodSpec if not serverless\n",
    "# import time # Needed for waiting\n",
    "\n",
    "# Ensure PINECONE_API_KEY, PINECONE_ENVIRONMENT, PINECONE_INDEX_NAME are loaded from Cell 2\n",
    "\n",
    "print(\"Initializing Pinecone client...\")\n",
    "# Use the Pinecone class for modern client initialization (v3+)\n",
    "# Pass api_key and environment using keyword arguments\n",
    "try:\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
    "    print(\"Pinecone client initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Pinecone client: {e}\")\n",
    "    print(\"Please check your PINECONE_API_KEY and PINECONE_ENVIRONMENT.\")\n",
    "    # Exit or raise an error if client initialization fails\n",
    "    raise e # Re-raise the exception\n",
    "\n",
    "\n",
    "# Define the dimension of the embeddings (text-embedding-ada-002 is 1536)\n",
    "embedding_dimension = 1536\n",
    "# Define the metric (cosine similarity is common for embeddings)\n",
    "metric = \"cosine\"\n",
    "\n",
    "# Define the index name\n",
    "# Use the index name loaded from environment variables (recommended)\n",
    "index_name = PINECONE_INDEX_NAME\n",
    "# Original code used index_name = \"langchain\" - ensure this matches your .env file or desired name\n",
    "\n",
    "print(f\"\\nChecking for Pinecone index: '{index_name}'\")\n",
    "\n",
    "# --- FIX for TypeError: argument of type 'method' is not iterable ---\n",
    "# Iterate through the list of index models returned by pc.list_indexes()\n",
    "# to check if an index with the desired name exists.\n",
    "index_exists = False\n",
    "try:\n",
    "    # pc.list_indexes() returns an object that is iterable, yielding IndexModel objects\n",
    "    for index_model in pc.list_indexes():\n",
    "        # Each IndexModel object has a 'name' attribute\n",
    "        if index_model.name == index_name:\n",
    "            index_exists = True\n",
    "            break # Found the index, no need to check further\n",
    "except Exception as e:\n",
    "    print(f\"Error listing Pinecone indexes: {e}\")\n",
    "    print(\"Could not verify if index exists. Please check your Pinecone connection and API key.\")\n",
    "    # If listing fails, we cannot reliably know if the index exists.\n",
    "    # We'll assume it doesn't exist and attempt creation, which might fail if it does.\n",
    "    index_exists = False\n",
    "\n",
    "\n",
    "if not index_exists:\n",
    "    print(f\"Index '{index_name}' does not exist. Creating it...\")\n",
    "    # Create the index - using ServerlessSpec as an example\n",
    "    # <--- IMPORTANT: Adjust the cloud and region based on your Pinecone account and plan! ---\n",
    "    try:\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=embedding_dimension,\n",
    "            metric=metric,\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\") # <--- ADJUST THIS SPEC\n",
    "            # If using pod-based, uncomment and adjust this:\n",
    "            # spec=PodSpec(environment=PINECONE_ENVIRONMENT, pod_type=\"p1.x1\") # <--- ADJUST THIS SPEC\n",
    "        )\n",
    "        # Wait for the index to be ready\n",
    "        print(\"Waiting for index to become ready...\")\n",
    "        while not pc.describe_index(index_name).status.ready: # Use .status.ready in v3+\n",
    "            time.sleep(1)\n",
    "        print(f\"Index '{index_name}' created and ready.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Pinecone index '{index_name}': {e}\")\n",
    "        print(\"Please check your Pinecone account, index name, dimensions, metric, and spec.\")\n",
    "        # Exit or raise error if index creation fails critically\n",
    "        raise e # Re-raise the exception to stop execution if index creation is essential\n",
    "\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists.\")\n",
    "\n",
    "# Connect to the index\n",
    "# Get an Index object to perform data operations (upsert, query, etc.)\n",
    "try:\n",
    "    index = pc.Index(index_name)\n",
    "    print(f\"\\nConnected to index '{index_name}'. Index stats:\")\n",
    "    # Use the index object to describe stats\n",
    "    print(index.describe_index_stats())\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Pinecone index '{index_name}': {e}\")\n",
    "    print(\"Please ensure the index name is correct and the index is ready.\")\n",
    "    # Exit or raise error if index connection fails\n",
    "    raise e # Re-raise the exception\n",
    "\n",
    "\n",
    "# Note: The 'index' object is the direct pinecone-client interface for data operations.\n",
    "# In the next step, we will use this with Langchain's PineconeVectorStore wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1a89185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting manual embedding and upserting process...\n",
      "Current vector count in index 'langchain': 17\n",
      "Index already contains vectors. Assuming data is already upserted.\n",
      "Connecting to existing Pinecone vector store 'langchain'...\n",
      "Connected to existing vector store.\n"
     ]
    }
   ],
   "source": [
    "# Cell 17 (Actual - Revised): Manually Embedding and Upserting Document Chunks to Pinecone\n",
    "\n",
    "# Ensure `pdf_chunks` is created from a previous cell (e.g., Cell 13 Actual)\n",
    "# Ensure `embeddings` (OpenAIEmbeddings instance) is initialized from a previous cell (e.g., Cell 15 Actual, or Cell 5 Actual)\n",
    "# Ensure the direct pinecone-client `index` object is initialized and connected (from Cell 16 Actual)\n",
    "\n",
    "print(\"Starting manual embedding and upserting process...\")\n",
    "\n",
    "# Define a batch size for upserting\n",
    "# Pinecone recommends batching for efficiency\n",
    "batch_size = 100 # Adjust based on chunk size and network; 100 is a common starting point\n",
    "\n",
    "# Check if the index already has vectors to avoid re-upserting the same data\n",
    "try:\n",
    "    index_stats = index.describe_index_stats()\n",
    "    print(f\"Current vector count in index '{index_name}': {index_stats.total_vector_count}\")\n",
    "\n",
    "    if index_stats.total_vector_count == 0:\n",
    "        print(\"Index appears empty. Proceeding with manual upsert.\")\n",
    "\n",
    "        # Iterate through chunks in batches\n",
    "        for i in range(0, len(pdf_chunks), batch_size):\n",
    "            # Get the current batch of chunks\n",
    "            batch_chunks = pdf_chunks[i:i + batch_size]\n",
    "            batch_end = min(i + batch_size, len(pdf_chunks))\n",
    "            print(f\"Processing batch {i} to {batch_end-1}...\")\n",
    "\n",
    "            # 1. Manually generate embeddings for the batch of chunks\n",
    "            # embeddings.embed_documents takes a list of strings\n",
    "            batch_texts = [chunk.page_content for chunk in batch_chunks]\n",
    "            try:\n",
    "                batch_embeddings = embeddings.embed_documents(batch_texts)\n",
    "                print(f\"  Generated {len(batch_embeddings)} embeddings for the batch.\")\n",
    "                if not batch_embeddings or len(batch_embeddings) != len(batch_chunks):\n",
    "                     print(f\"  Warning: Embedding batch returned unexpected results (length {len(batch_embeddings)}). Skipping upsert for this batch.\")\n",
    "                     continue # Skip this batch if embedding failed or was incomplete\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Error generating embeddings for batch {i}: {e}. Skipping upsert for this batch.\")\n",
    "                continue # Skip this batch if embedding failed\n",
    "\n",
    "\n",
    "            # 2. Prepare data for Pinecone upsert\n",
    "            # Pinecone upsert expects a list of (id, vector, metadata) tuples or dictionaries\n",
    "            vectors_to_upsert = []\n",
    "            for j, chunk in enumerate(batch_chunks):\n",
    "                # Generate a unique ID for each chunk\n",
    "                # A simple approach: combine source, page, and chunk index within the page\n",
    "                # You might need a more robust ID generation strategy for large datasets\n",
    "                chunk_id = f\"{chunk.metadata.get('source', 'unknown')}-{chunk.metadata.get('page', 0)}-chunk{i+j}\"\n",
    "\n",
    "                # Get the corresponding embedding vector\n",
    "                vector = batch_embeddings[j]\n",
    "\n",
    "                # Get the metadata (Pinecone stores metadata as a dictionary)\n",
    "                metadata = chunk.metadata # Use the existing metadata from the Document\n",
    "\n",
    "                vectors_to_upsert.append((chunk_id, vector, metadata))\n",
    "\n",
    "            # 3. Perform batch upsert using the direct pinecone-client index object\n",
    "            if vectors_to_upsert:\n",
    "                try:\n",
    "                    upsert_response = index.upsert(vectors=vectors_to_upsert)\n",
    "                    print(f\"  Batch upsert response: {upsert_response}\")\n",
    "                except Exception as e:\n",
    "                     print(f\"  Error during Pinecone upsert for batch {i}: {e}. Continuing to next batch.\")\n",
    "                     # Don't raise here, try to process remaining batches\n",
    "            else:\n",
    "                 print(f\"  No vectors prepared for upsert in batch {i}.\")\n",
    "\n",
    "\n",
    "        print(\"\\nManual upsert process finished.\")\n",
    "        # Get final index stats via the direct pinecone client index object\n",
    "        # Add a short delay to allow Pinecone stats to update\n",
    "        print(\"Waiting for index stats to update...\")\n",
    "        time.sleep(10)\n",
    "        print(f\"Final vector count after manual upsert: {index.describe_index_stats().total_vector_count}\")\n",
    "\n",
    "        # We still need a Langchain PineconeVectorStore instance for retrieval later\n",
    "        # Load the vector store instance from the existing index\n",
    "        print(\"Initializing Langchain PineconeVectorStore from existing index...\")\n",
    "        vectorstore = PineconeVectorStore.from_existing_index(\n",
    "            index_name=index_name,\n",
    "            embedding=embeddings, # Pass the embedding model used for this index\n",
    "            # You might need to pass the text_key if your metadata field name for text is not 'text'\n",
    "            # text_key=\"page_content\" # Assuming 'page_content' is stored in metadata field named 'text' by default from_documents, might need adjustment here\n",
    "        )\n",
    "        print(\"Langchain PineconeVectorStore initialized from existing index.\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Index already contains vectors. Assuming data is already upserted.\")\n",
    "        # If index is not empty, initialize the vector store instance from the existing index\n",
    "        print(f\"Connecting to existing Pinecone vector store '{index_name}'...\")\n",
    "        vectorstore = PineconeVectorStore.from_existing_index(\n",
    "            index_name=index_name,\n",
    "            embedding=embeddings, # Pass the embedding model used for this index\n",
    "             # text_key=\"page_content\" # Adjust text_key if needed\n",
    "        )\n",
    "        print(\"Connected to existing vector store.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during Pinecone upsert or connection: {e}\")\n",
    "    print(\"Please check your Pinecone setup, index name, and that pdf_chunks/embeddings are correct.\")\n",
    "    # Exit or raise error if this critical step fails\n",
    "    raise e # Re-raise the exception\n",
    "\n",
    "\n",
    "# The 'vectorstore' object (an instance of PineconeVectorStore) is now ready for retrieval (similarity search).\n",
    "# You will use vectorstore.as_retriever() in subsequent query steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05676366",
   "metadata": {},
   "source": [
    "### Testing APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bb0a417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debugging: Testing OpenAI Embeddings directly...\n",
      "Successfully embedded 2 sentences.\n",
      "First test vector dimension: 1536\n",
      "Sample: [-0.0011391325388103724, -0.003206387162208557, 0.002380132209509611, -0.004501554183661938, -0.010328996926546097]...\n"
     ]
    }
   ],
   "source": [
    "# Debugging Step: Test OpenAI Embeddings separately\n",
    "# Ensure 'embeddings' object is initialized from a previous cell\n",
    "print(\"\\nDebugging: Testing OpenAI Embeddings directly...\")\n",
    "try:\n",
    "    test_texts = [\"This is a test sentence.\", \"This is another test sentence.\"]\n",
    "    test_vectors = embeddings.embed_documents(test_texts)\n",
    "    print(f\"Successfully embedded {len(test_vectors)} sentences.\")\n",
    "    if test_vectors:\n",
    "         print(f\"First test vector dimension: {len(test_vectors[0])}\") # Should be 1536\n",
    "         print(f\"Sample: {test_vectors[0][:5]}...\")\n",
    "    else:\n",
    "         print(\"Embedding returned an empty list.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during direct embedding test: {e}\")\n",
    "    print(\"Potential issue with OpenAI API key or connection for embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc5594a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debugging: Testing direct Pinecone upsert...\n",
      "Direct Pinecone upsert response: {'upserted_count': 1}\n",
      "Vector count after direct upsert test: 17\n"
     ]
    }
   ],
   "source": [
    "# Debugging Step: Test Pinecone Upsert directly\n",
    "# Ensure 'index' object (direct pinecone-client index) is initialized from Cell 16 Actual\n",
    "# Ensure 'embeddings' object is initialized from a previous cell\n",
    "\n",
    "print(\"\\nDebugging: Testing direct Pinecone upsert...\")\n",
    "try:\n",
    "    # Embed a simple test string to get a valid vector\n",
    "    test_string_for_upsert = \"Pinecone direct upsert test.\"\n",
    "    test_vector_for_upsert = embeddings.embed_query(test_string_for_upsert) # Use embed_query for a single string\n",
    "\n",
    "    if test_vector_for_upsert and len(test_vector_for_upsert) == 1536: # Check if embedding succeeded\n",
    "        # Define a unique ID for the test vector\n",
    "        test_vector_id = \"test-vector-12345\"\n",
    "        # Define metadata for the test vector\n",
    "        test_metadata = {\"source\": \"debug\", \"text\": test_string_for_upsert}\n",
    "\n",
    "        # Structure the data for upsert (list of tuples or dicts)\n",
    "        vectors_to_upsert = [(test_vector_id, test_vector_for_upsert, test_metadata)] # (id, vector, metadata) format\n",
    "\n",
    "        # Perform the upsert\n",
    "        upsert_response = index.upsert(vectors=vectors_to_upsert)\n",
    "        print(f\"Direct Pinecone upsert response: {upsert_response}\")\n",
    "\n",
    "        # Check stats after a brief delay\n",
    "        time.sleep(5) # Give Pinecone a few seconds to update stats\n",
    "        updated_stats = index.describe_index_stats()\n",
    "        print(f\"Vector count after direct upsert test: {updated_stats.total_vector_count}\")\n",
    "\n",
    "        # Optional: Clean up the test vector\n",
    "        # print(\"Deleting test vector...\")\n",
    "        # index.delete(ids=[test_vector_id])\n",
    "        # time.sleep(5)\n",
    "        # print(f\"Vector count after deletion attempt: {index.describe_index_stats().total_vector_count}\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to generate test vector for direct upsert.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during direct Pinecone upsert test: {e}\")\n",
    "    print(\"Potential issue with Pinecone connection, API key, or index state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e9cc35",
   "metadata": {},
   "source": [
    "### Natural Language Retrieval\n",
    "We first start performing a semantic search within our Vector DataBase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9355feda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing semantic search in the FAISS vector store.\n",
      "\n",
      "Searching FAISS for top 2 matches for query: 'Can you please tell me all the authors of the article Attention is all you need?'\n",
      "\n",
      "Found 2 similar document chunks:\n",
      "--- Match 1 ---\n",
      "Source: Docs/attentions.pdf, Page: 0\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Par...\n",
      "--- Match 2 ---\n",
      "Source: Docs/attentions.pdf, Page: 13\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Cell 18 (Actual): Performing Semantic Search in FAISS\n",
    "\n",
    "# Remove redundant initialization - db_FAISS and embeddings should already exist\n",
    "# df = FAISS.from_documents(pdf_chunks, embeddings) # This line recreates the index, likely not intended here\n",
    "# embeddings = OpenAIEmbeddings() # Embeddings should be initialized earlier\n",
    "\n",
    "print(\"Performing semantic search in the FAISS vector store.\")\n",
    "\n",
    "# Define the query for semantic search\n",
    "query = \"Can you please tell me all the authors of the article Attention is all you need?\" # <--- Your search query\n",
    "\n",
    "# We can define how many similar results we want to get back by defining the variable k\n",
    "k_value = 2 # Number of top similar documents to retrieve\n",
    "\n",
    "# Perform the similarity search on the FAISS database\n",
    "# Use the .similarity_search() method\n",
    "print(f\"\\nSearching FAISS for top {k_value} matches for query: '{query}'\")\n",
    "matches = db_FAISS.similarity_search(query, k=k_value)\n",
    "\n",
    "print(f\"\\nFound {len(matches)} similar document chunks:\")\n",
    "\n",
    "# Print the retrieved document chunks\n",
    "if matches:\n",
    "    for i, doc in enumerate(matches):\n",
    "        print(f\"--- Match {i+1} ---\")\n",
    "        print(f\"Source: {doc.metadata.get('source')}, Page: {doc.metadata.get('page')}\")\n",
    "        # Print a snippet of the content\n",
    "        print(doc.page_content[:300] + \"...\")\n",
    "        # Optional: Print the score if available (FAISS.similarity_search doesn't return score by default)\n",
    "        # If you need scores, use similarity_search_with_score\n",
    "        # print(f\"Score: {score}\")\n",
    "else:\n",
    "    print(\"No matches found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70513362",
   "metadata": {},
   "source": [
    "In the above section, we have seen how to retrieve the coincidences of you query in the documents in our vector store. Nevertheless, the output is a bit difficult to read. We can leverage the usage of LLMs by feeding the coincidences in our vector store to an LLM and let it generate a response in Natural Language using the additional information from our documents. We can do so by using the so-called **[LangChain Chains](https://python.langchain.com/docs/expression_language/get_started)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17180e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up RAG chain for Pinecone with top 2 matches.\n",
      "\n",
      "Running RAG Chain with the query...\n",
      "\n",
      "Answer:\n",
      "The authors of the article \"Attention Is All You Need\" are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.\n",
      "\n",
      "Retrieved Context:\n",
      "--- Document 1 ---\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "...\n",
      "Source: Docs/attentions.pdf, Page: 0.0\n",
      "--- Document 2 ---\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "sh...\n",
      "Source: Docs/attentions.pdf, Page: 13.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 20 (Actual): Performing RAG Q&A using Pinecone\n",
    "\n",
    "# Remove the outdated import for load_qa_chain\n",
    "# from langchain.chains.Youtubeing import load_qa_chain\n",
    "\n",
    "# Ensure `vectorstore` (PineconeVectorStore instance) is initialized from Cell 17 Actual\n",
    "# Ensure `chatgpt` or `llm` (ChatOpenAI instance) is initialized from a previous cell (e.g., Cell 3 or 4)\n",
    "# Ensure necessary imports for modern chains (from Cell 1) are available:\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain.chains import create_retrieval_chain\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "# from langchain_core.runnables import RunnablePassthrough, RunnableParallel # Needed for LCEL\n",
    "# from langchain_core.output_parsers import StrOutputParser # Needed for LCEL\n",
    "\n",
    "# Define a query for semantic search\n",
    "query = \"Can you please tell me all the authors of the article Attention is all you need?\" # <--- Your search query\n",
    "\n",
    "# We can define how many similar results we want to get back by defining the variable k\n",
    "k_value = 2 # Number of top similar documents to retrieve (original code used 2, let's keep that)\n",
    "\n",
    "# Ensure `vectorstore` (PineconeVectorStore) is initialized and ready for use\n",
    "# This is the Langchain wrapper instance connected to your Pinecone index\n",
    "if 'vectorstore' not in locals() or vectorstore is None:\n",
    "    print(\"Error: Pinecone vector store (vectorstore) not found. Please run Cell 17 Actual first.\")\n",
    "else:\n",
    "    print(f\"Setting up RAG chain for Pinecone with top {k_value} matches.\")\n",
    "\n",
    "    # 1. Get a Retriever from the Pinecone Vector Store\n",
    "\n",
    "    # Get a retriever instance from the Pinecone vector store.\n",
    "    # This object is capable of performing the similarity search on Pinecone.\n",
    "    # Pass k to the retriever using search_kwargs\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k_value})\n",
    "\n",
    "    # You can perform the search explicitly to inspect results (optional):\n",
    "    # matches = retriever.invoke(query)\n",
    "    # print(f\"Retrieved {len(matches)} document chunks from Pinecone.\")\n",
    "    # if matches:\n",
    "    #     print(f\"Sample match source: {matches[0].metadata.get('source')}, page: {matches[0].metadata.get('page')}\")\n",
    "\n",
    "\n",
    "    # 2. Define the Prompt Template for the LLM\n",
    "\n",
    "    # This prompt instructs the LLM on how to use the retrieved context to answer the question.\n",
    "    # Use ChatPromptTemplate for chat models.\n",
    "    # (Import ChatPromptTemplate is already in Cell 1)\n",
    "    rag_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Answer the user's question based on the below context:\\n\\n{context}\\n\\nIf the answer is not in the context, say 'I cannot answer that question based on the provided information.'\"),\n",
    "        (\"user\", \"{question}\")\n",
    "    ])\n",
    "\n",
    "    # 3. Create the Chain to Combine Documents and Answer using create_stuff_documents_chain\n",
    "\n",
    "    # This utility handles formatting the list of Documents into a string for the prompt's {context}.\n",
    "    # It expects 'context' (list of Docs) and 'question' (string) as input.\n",
    "    # (Import create_stuff_documents_chain is already in Cell 1)\n",
    "    combine_docs_chain = create_stuff_documents_chain(\n",
    "        chatgpt,           # Pass your Chat LLM instance (assuming 'chatgpt' or 'llm')\n",
    "        rag_prompt_template # Pass the RAG prompt template\n",
    "    )\n",
    "\n",
    "    # 4. Create the Full RAG Chain using create_retrieval_chain\n",
    "\n",
    "    # This utility connects the retriever and the combine_docs_chain.\n",
    "    # It is designed to take {\"question\": ...} as input, run the retriever with the query,\n",
    "    # get the context, and pass {\"context\": [...], \"question\": ...} to the combine_docs_chain.\n",
    "    # (Import create_retrieval_chain is already in Cell 1)\n",
    "    rag_chain = create_retrieval_chain(\n",
    "        retriever,          # The retriever gets the relevant documents from Pinecone\n",
    "        combine_docs_chain  # The combine_docs_chain uses the documents and question to answer\n",
    "    )\n",
    "\n",
    "    print(\"\\nRunning RAG Chain with the query...\")\n",
    "\n",
    "    # 5. Invoke the RAG Chain\n",
    "\n",
    "    # Invoke the RAG chain with the query.\n",
    "    # According to documentation and typical usage, create_retrieval_chain expects {\"question\": ...} as input.\n",
    "    response = rag_chain.invoke({\n",
    "        \"input\":   query,   # so you satisfy the top‐level chain\n",
    "        \"question\": query   # so the combine prompt has its {question} value\n",
    "    }) # <--- Using 'question' key\n",
    "\n",
    "\n",
    "    # The response object from create_retrieval_chain is a dictionary.\n",
    "    # The final answer is typically under the 'answer' key.\n",
    "    # The retrieved documents are available under the 'context' key.\n",
    "    print(\"\\nAnswer:\")\n",
    "    # Use .get() for safe access in case 'answer' key is missing, although it should be present\n",
    "    print(response.get('answer', 'No answer generated.'))\n",
    "\n",
    "    # Also print retrieved context explicitly, as it's returned by create_retrieval_chain\n",
    "    print(\"\\nRetrieved Context:\")\n",
    "    if 'context' in response:\n",
    "        for i, doc in enumerate(response['context']):\n",
    "            print(f\"--- Document {i+1} ---\")\n",
    "            # Print snippet and metadata for the retrieved docs\n",
    "            print(doc.page_content[:200] + \"...\")\n",
    "            print(f\"Source: {doc.metadata.get('source')}, Page: {doc.metadata.get('page')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9032c5dd",
   "metadata": {},
   "source": [
    "### Indexes and Metadata\n",
    "When we upload data to our vector database, there is metadata that allows us to understand where the data is coming from. \n",
    "When dealing with PDFs, the source information allows us to know what pdf and page the info is coming from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ebebea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing semantic search in FAISS and inspecting results.\n",
      "\n",
      "Searching FAISS for top 4 matches for query: 'Who created transformers?'\n",
      "Found 4 similar document chunks.\n",
      "\n",
      "______________________________________ MATCH 4 (Index 3)\n",
      "\n",
      "Chunk text content:\n",
      "1 Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [38, 24, 15].\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "significant improvements in computational efficiency through factorization tricks [21] and conditional\n",
      "computation [32], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\n",
      "of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\n",
      "sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n",
      "[10], consuming the previously generated symbols as additional input when generating the next.\n",
      "2\n",
      "\n",
      "Chunk metadata:\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Docs/attentions.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}\n",
      "\n",
      "The source of our match is: Docs/attentions.pdf, and page: 1\n"
     ]
    }
   ],
   "source": [
    "# Cell 21 (Actual): Inspecting Retrieved Documents from FAISS Search\n",
    "\n",
    "# Ensure `db_FAISS` (FAISS vector store instance) is created from Cell 15 Actual\n",
    "# Ensure `embeddings` (OpenAIEmbeddings instance) is initialized from a previous cell\n",
    "\n",
    "print(\"Performing semantic search in FAISS and inspecting results.\")\n",
    "\n",
    "# Define the query for semantic search\n",
    "query = \"Who created transformers?\" # <--- Your search query\n",
    "\n",
    "# We can define how many similar results we want to get back by defining the variable k\n",
    "# Note: Original code didn't specify k, which defaults to 4.\n",
    "# However, if fewer than 4 matches are found, accessing matches[3] will cause an IndexError.\n",
    "# It's best practice to specify k if you intend to access a specific index.\n",
    "k_value = 4 # Set k to at least the index you intend to access (index 3 requires k>=4)\n",
    "\n",
    "# Ensure db_FAISS exists and is ready for use\n",
    "if 'db_FAISS' not in locals() or db_FAISS is None:\n",
    "    print(\"Error: FAISS vector store (db_FAISS) not found. Please run Cell 15 Actual first.\")\n",
    "else:\n",
    "    print(f\"\\nSearching FAISS for top {k_value} matches for query: '{query}'\")\n",
    "    # Perform the similarity search on the FAISS database using its retriever\n",
    "    # Get a retriever instance from the FAISS database\n",
    "    # Pass k to the retriever using search_kwargs, or directly to similarity_search if preferred\n",
    "    # Using similarity_search directly as in the original code:\n",
    "    matches = db_FAISS.similarity_search(query, k=k_value)\n",
    "\n",
    "\n",
    "    print(f\"Found {len(matches)} similar document chunks.\")\n",
    "\n",
    "    # Check if the desired match index exists before trying to access it\n",
    "    target_match_index = 3 # Original code accessed index 3 (the 4th match)\n",
    "\n",
    "    print(f\"\\n______________________________________ MATCH {target_match_index + 1} (Index {target_match_index})\")\n",
    "\n",
    "    if len(matches) > target_match_index:\n",
    "        # Access the specific match (index 3)\n",
    "        target_match = matches[target_match_index]\n",
    "\n",
    "        # Print the chunk text content\n",
    "        print(\"\\nChunk text content:\")\n",
    "        print(target_match.page_content) # Use .page_content\n",
    "\n",
    "        # Print the chunk metadata\n",
    "        print(\"\\nChunk metadata:\")\n",
    "        print(target_match.metadata) # Use .metadata (a dictionary)\n",
    "\n",
    "        # Print specific metadata fields\n",
    "        # Use .get() for safe access in case the key is missing\n",
    "        source = target_match.metadata.get(\"source\", \"N/A\")\n",
    "        page = target_match.metadata.get(\"page\", \"N/A\") # Assuming 'page' is the key for page number\n",
    "        print(f\"\\nThe source of our match is: {source}, and page: {page}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: Fewer than {k_value} matches found. Cannot access match at index {target_match_index}.\")\n",
    "        print(f\"Only {len(matches)} matches were returned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fafcdea",
   "metadata": {},
   "source": [
    "Now it is the time to put it all together and generate a simple pipeline to query our documents using a LLM model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19f6d5",
   "metadata": {},
   "source": [
    "# PART 2: Loading and processing our documents\n",
    "\n",
    "\n",
    "\n",
    "PyPDFDirectoryLoader allows us to upload multiple PDFs at once. In our case, we have two PDFs in the Docs directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc3214",
   "metadata": {},
   "source": [
    "## **STEP 1 - LOADER**\n",
    "\n",
    "Use the `PDFDirectoryLoader` to upload all PDFs contained within the the Docs folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b143a827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from a directory using PyPDFDirectoryLoader.\n",
      "Loading content from directory: Docs/\n",
      "Successfully loaded 101 document pages from directory Docs/.\n"
     ]
    }
   ],
   "source": [
    "# Cell X: Load Documents from a Directory\n",
    "\n",
    "# Import the necessary loader using its modern path\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "print(\"Loading documents from a directory using PyPDFDirectoryLoader.\")\n",
    "\n",
    "# Define the path to your directory containing PDFs\n",
    "# <--- IMPORTANT: CHANGE THIS PATH to YOUR directory containing PDFs ---\n",
    "pdf_directory_path = \"Docs/\" # Example path from original code\n",
    "\n",
    "print(f\"Loading content from directory: {pdf_directory_path}\")\n",
    "\n",
    "try:\n",
    "    # Initialize the PyPDFDirectoryLoader\n",
    "    loader = PyPDFDirectoryLoader(pdf_directory_path)\n",
    "\n",
    "    # Load documents from the directory. This returns a list of Document objects.\n",
    "    # Each Document will typically be a page from one of the PDFs in the directory.\n",
    "    data = loader.load() # Using 'data' as the variable name as in your snippet\n",
    "\n",
    "    print(f\"Successfully loaded {len(data)} document pages from directory {pdf_directory_path}.\")\n",
    "\n",
    "    # Inspect the loaded data (optional)\n",
    "    # if data:\n",
    "    #     print(\"\\nFirst loaded document snippet:\")\n",
    "    #     print(f\"Source: {data[0].metadata.get('source')}, Page: {data[0].metadata.get('page')}\")\n",
    "    #     print(data[0].page_content[:500] + \"...\") # Print first 500 chars\n",
    "    # else:\n",
    "    #      print(\"No data was loaded from the directory.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The directory '{pdf_directory_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading PDFs from directory: {e}\")\n",
    "\n",
    "# The 'data' variable now holds the list of Document objects loaded from the PDFs.\n",
    "# This list will be used in subsequent steps for splitting, embedding, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1649a99a",
   "metadata": {},
   "source": [
    "## **STEP 2 - CHUNKING**\n",
    "\n",
    "Generate the chunks for the PDFs contained in the directory. \n",
    "1. Import both RecursiveCharacterTextSplitter from langchain.text_splitter and GPT2TokenizerFast from transformers. \n",
    "2. Define a tokenizer with the following command: tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "3. Define a count_tokens function that will allow us to count the tokens of out text. \n",
    "4. Define the text_splitter with a chunk_size of 200, a chunk_overlap of 20 and the length_function we have just defined. \n",
    "5. Apply the command `.split_documents`to our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a52761e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking loaded document data.\n",
      "tiktoken tokenizer initialized for chunking.\n",
      "\n",
      "Initializing RecursiveCharacterTextSplitter...\n",
      "\n",
      "Splitting 101 document pages into chunks...\n",
      "Chunking complete - Now you have 584 number of chunks.\n"
     ]
    }
   ],
   "source": [
    "# Cell X: Chunking Our Data\n",
    "\n",
    "# Import necessary components (imports already in Cell 1)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from transformers import GPT2TokenizerFast # Original import - we'll use tiktoken instead\n",
    "import tiktoken # Recommended tokenizer for OpenAI models\n",
    "\n",
    "# Ensure `data` is loaded from the previous cell (Step 1 in this section)\n",
    "\n",
    "print(\"Chunking loaded document data.\")\n",
    "\n",
    "# Use tiktoken for token counting with OpenAI models\n",
    "# Get the tokenizer encoding for the embedding model you plan to use (e.g., text-embedding-ada-002)\n",
    "# 'cl100k_base' is common for modern OpenAI models\n",
    "try:\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    print(\"tiktoken tokenizer initialized for chunking.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing tiktoken tokenizer: {e}\")\n",
    "    print(\"Falling back to character count for splitting.\")\n",
    "    tokenizer = None # Set to None if initialization fails\n",
    "\n",
    "# Create function to count tokens using tiktoken\n",
    "def count_tokens_tiktoken(text: str) -> int:\n",
    "    if tokenizer:\n",
    "        tokens = tokenizer.encode(text, disallowed_special=())\n",
    "        return len(tokens)\n",
    "    return len(text) # Fallback to character count if tiktoken failed\n",
    "\n",
    "\n",
    "# Define the splitter\n",
    "# Use chunk_size and chunk_overlap in tokens\n",
    "chunk_size = 200 # Example size from original code - adjust as needed\n",
    "chunk_overlap = 20 # Example overlap from original code - adjust as needed\n",
    "\n",
    "print(\"\\nInitializing RecursiveCharacterTextSplitter...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=count_tokens_tiktoken if tokenizer else len, # Use tiktoken function if available, else len\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"] # Common separators\n",
    ")\n",
    "\n",
    "# Apply the .split_document command\n",
    "if 'data' in locals() and data: # Check if data variable exists and is not empty\n",
    "    print(f\"\\nSplitting {len(data)} document pages into chunks...\")\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    print(f\"Chunking complete - Now you have {len(chunks)} number of chunks.\")\n",
    "\n",
    "    # Optional: Inspect a chunk\n",
    "    # if chunks:\n",
    "    #     print(\"\\nFirst chunk snippet:\")\n",
    "    #     print(chunks[0].page_content[:500] + \"...\")\n",
    "    #     print(f\"Metadata: {chunks[0].metadata}\")\n",
    "    #     print(f\"Token count (tiktoken): {count_tokens_tiktoken(chunks[0].page_content)}\")\n",
    "else:\n",
    "    print(\"data not found or is empty. Skipping chunking.\")\n",
    "\n",
    "# The 'chunks' variable now holds the list of Document objects representing the text chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc5b37e",
   "metadata": {},
   "source": [
    "## **STEP 3 - EMBEDD AND UPLOAD THE DATA INTO A VECTORSTORE**\n",
    "\n",
    "**TASK**\n",
    "- Upload the data into the FAISS vector store using the `from_documents`command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1810658d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS vector store from chunks and testing semantic search.\n",
      "\n",
      "Creating FAISS vector database from chunks...\n",
      "FAISS vector database created (db_FAISS).\n",
      "\n",
      "Testing similarity search with query: 'Who created transformers?'\n",
      "We found 4 number of similarities.\n",
      "Top matches:\n",
      "\n",
      "--- Match 1 ---\n",
      "Source: Docs\\codex.pdf, Page: 15\n",
      "training of deep bidirectional transformers for language under-\n",
      "standing. arXiv preprint arXiv:1810.04805, 2018.\n",
      "Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A., and\n",
      "Sutskever, I. Jukebox: A generative model for music. arXiv\n",
      "preprint arXiv:2005.00341, 2020.\n",
      "Drain, D., Wu, C., Svyatkovskiy,...\n",
      "\n",
      "--- Match 2 ---\n",
      "Source: Docs\\attentions.pdf, Page: 0\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution....\n",
      "\n",
      "--- Match 3 ---\n",
      "Source: Docs\\codegen.pdf, Page: 8\n",
      "to natural language processing (Devlin et al., 2019; Lewis et al., 2020; Raffel et al., 2020), computer\n",
      "vision (Dosovitskiy et al., 2021), and many other areas (Oord et al., 2018; Jumper et al., 2021). Prior\n",
      "works, such as CuBERT (Kanade et al., 2020), CodeBERT (Feng et al., 2020), PyMT5 (Clement et...\n",
      "\n",
      "--- Match 4 ---\n",
      "Source: Docs\\attentions.pdf, Page: 0\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple...\n"
     ]
    }
   ],
   "source": [
    "# Cell X: Embed and Upload Data into FAISS Vector Store\n",
    "\n",
    "# Import necessary vector store component (import already in Cell 1 or 15)\n",
    "# from langchain.vectorstores import FAISS # Original import path\n",
    "from langchain_community.vectorstores import FAISS # Modern import path\n",
    "\n",
    "# Ensure `chunks` is created from the previous cell (Step 2 in this section)\n",
    "# Ensure `embeddings` (OpenAIEmbeddings instance) is initialized from a previous cell (e.g., Cell 5 Actual, or 15 Actual)\n",
    "\n",
    "print(\"Creating FAISS vector store from chunks and testing semantic search.\")\n",
    "\n",
    "# ___________________________________________________________________________ LOCAL VERSION (FAISS)\n",
    "\n",
    "# 1. Create vector database with FAISS\n",
    "# Use the FAISS.from_documents class method.\n",
    "# This method generates embeddings for the chunks using the provided embeddings model\n",
    "# and builds the FAISS index.\n",
    "print(\"\\nCreating FAISS vector database from chunks...\")\n",
    "if 'chunks' in locals() and chunks and 'embeddings' in locals() and embeddings:\n",
    "    db_FAISS = FAISS.from_documents(chunks, embeddings)\n",
    "    print(\"FAISS vector database created (db_FAISS).\")\n",
    "\n",
    "    # Optional: You can save the FAISS index to disk if you want to reuse it later\n",
    "    # faiss_save_path = \"./my_faiss_chunks_index\"\n",
    "    # print(f\"Saving FAISS index to {faiss_save_path}...\")\n",
    "    # db_FAISS.save_local(faiss_save_path)\n",
    "    # print(\"FAISS index saved.\")\n",
    "\n",
    "    # 2. Check similarity search is working\n",
    "    # Define a query\n",
    "    query = \"Who created transformers?\" # <--- Your test query\n",
    "\n",
    "    # Perform the semantic search in our vector database with the similarity_search command.\n",
    "    # We can define how many similarities we want to get back by defining the variable k\n",
    "    k_value = 4 # Number of top similarities to get\n",
    "\n",
    "    print(f\"\\nTesting similarity search with query: '{query}'\")\n",
    "    matches = db_FAISS.similarity_search(query, k=k_value) # Specify k here\n",
    "\n",
    "\n",
    "    print(f\"We found {len(matches)} number of similarities.\")\n",
    "    # Print the content of the top matches\n",
    "    if matches:\n",
    "        print(\"Top matches:\")\n",
    "        # Limit the printout for brevity\n",
    "        for i, match in enumerate(matches):\n",
    "             print(f\"\\n--- Match {i+1} ---\")\n",
    "             print(f\"Source: {match.metadata.get('source')}, Page: {match.metadata.get('page')}\")\n",
    "             print(match.page_content[:300] + \"...\") # Print first 300 chars\n",
    "    else:\n",
    "        print(\"No matches found in FAISS search.\")\n",
    "\n",
    "else:\n",
    "    print(\"Chunks or Embeddings not found or are empty. Skipping FAISS database creation and search test.\")\n",
    "\n",
    "# The 'db_FAISS' variable now holds the FAISS vector store instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e64fb4",
   "metadata": {},
   "source": [
    "# PART 3: Talking with our documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda2673",
   "metadata": {},
   "source": [
    "## STEP 4 - DEFINE A CHAIN AND PERFORM THE SIMILARITY SEARCH\n",
    "Generating a simple pipeline to query our documents with a load_qa_chain. \n",
    "**TASK**\n",
    "1. Import the `load_qa_chain`from the langchain.chains.question_answering library. \n",
    "2. Define a prompt of interest, like: \"Can you please tell me all the autors of the article Attention is all you need?\"\n",
    "3. Define the chain.\n",
    "4. Perform a semantic search with the `.similarity_search`. \n",
    "5. Execute the chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e6076a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up and running Basic RAG Q&A using FAISS.\n",
      "\n",
      "Performing semantic search in FAISS for top 1 matches for query: 'Can you please tell me all the autors of the article Attention is all you need?'\n",
      "\n",
      "Executing the RAG chain...\n",
      "\n",
      "Answer:\n",
      "The authors of the article \"Attention is all you need\" are Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I.\n"
     ]
    }
   ],
   "source": [
    "# Cell X: Basic RAG Q&A using FAISS (load_qa_chain Update)\n",
    "\n",
    "# Remove the outdated import for load_qa_chain\n",
    "# from langchain.chains.Youtubeing import load_qa_chain\n",
    "\n",
    "# Ensure `db_FAISS` (FAISS vector store instance) is created from a previous cell (e.g., Step 3 in this section)\n",
    "# Ensure `chatgpt` or `llm` (ChatOpenAI instance) is initialized from a previous cell (e.g., Cell 3 or 4 Actual)\n",
    "# Ensure necessary imports for modern chains (from Cell 1) are available:\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain.chains import create_retrieval_chain\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "# from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda # Needed for LCEL\n",
    "# from langchain_core.output_parsers import StrOutputParser # Needed for LCEL\n",
    "\n",
    "print(\"Setting up and running Basic RAG Q&A using FAISS.\")\n",
    "\n",
    "# 1. Define a query of interest.\n",
    "query = \"Can you please tell me all the autors of the article Attention is all you need?\" # <--- Your search query\n",
    "\n",
    "# We can define how many similar results we want to get back by defining the variable k\n",
    "k_value = 1 # Number of top similar documents to retrieve (original code used 1)\n",
    "\n",
    "# Ensure db_FAISS exists and is ready for use\n",
    "if 'db_FAISS' not in locals() or db_FAISS is None:\n",
    "    print(\"Error: FAISS vector store (db_FAISS) not found. Please run the cell creating db_FAISS first.\")\n",
    "elif 'chatgpt' not in locals() or chatgpt is None:\n",
    "     print(\"Error: ChatOpenAI model (chatgpt) not found. Please run a cell initializing chatgpt first.\")\n",
    "else:\n",
    "    print(f\"\\nPerforming semantic search in FAISS for top {k_value} matches for query: '{query}'\")\n",
    "\n",
    "    # Get a retriever instance from the FAISS database, configured with k\n",
    "    retriever = db_FAISS.as_retriever(search_kwargs={\"k\": k_value})\n",
    "\n",
    "    # You can perform the search explicitly to inspect results (optional):\n",
    "    # matches = retriever.invoke(query)\n",
    "    # print(f\"Retrieved {len(matches)} document chunks from FAISS.\")\n",
    "    # if matches:\n",
    "    #     print(f\"Sample match source: {matches[0].metadata.get('source')}, page: {matches[0].metadata.get('page')}\")\n",
    "\n",
    "\n",
    "    # --- Replacing load_qa_chain with modern LCEL-based RAG Chain ---\n",
    "\n",
    "    # 2. Define the Prompt Template for the LLM\n",
    "    # This prompt instructs the LLM on how to use the retrieved context to answer the question.\n",
    "    # Use ChatPromptTemplate for chat models.\n",
    "    # (Import ChatPromptTemplate is already in Cell 1)\n",
    "    rag_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Answer the user's question based on the below context:\\n\\n{context}\\n\\nIf the answer is not in the context, say 'I cannot answer that question based on the provided information.'\"),\n",
    "        (\"user\", \"{question}\")\n",
    "    ])\n",
    "\n",
    "    # 3. Create the Chain to Combine Documents and Answer using create_stuff_documents_chain\n",
    "    # This utility handles formatting the list of Documents into a string for the prompt's {context}.\n",
    "    # It expects 'context' (list of Docs) and 'question' (string) as input.\n",
    "    # (Import create_stuff_documents_chain is already in Cell 1)\n",
    "    combine_docs_chain = create_stuff_documents_chain(\n",
    "        chatgpt,           # Pass your Chat LLM instance\n",
    "        rag_prompt_template # Pass the RAG prompt template\n",
    "    )\n",
    "\n",
    "    # 4. Create the Full RAG Chain using create_retrieval_chain\n",
    "    # This utility connects the retriever and the combine_docs_chain.\n",
    "    # It is designed to take {\"question\": ...} as input, run the retriever with the query,\n",
    "    # get the context, and pass {\"context\": [...], \"question\": ...} to the combine_docs_chain.\n",
    "    # (Import create_retrieval_chain is already in Cell 1)\n",
    "    rag_chain = create_retrieval_chain(\n",
    "        retriever,          # The retriever gets the relevant documents from FAISS\n",
    "        combine_docs_chain  # The combine_docs_chain uses the documents and question to answer\n",
    "    )\n",
    "\n",
    "    print(\"\\nExecuting the RAG chain...\")\n",
    "\n",
    "    # 5. Execute the chain to obtain a response.\n",
    "    # Invoke the RAG chain with the query.\n",
    "    # create_retrieval_chain expects {\"question\": ...} as input.\n",
    "    response = rag_chain.invoke({\n",
    "    \"input\":   query,   # so you satisfy the top‐level chain\n",
    "    \"question\": query   # so the combine prompt has its {question} value\n",
    "}) # <--- Using 'question' key\n",
    "\n",
    "\n",
    "    # The response object from create_retrieval_chain is a dictionary.\n",
    "    # The final answer is typically under the 'answer' key.\n",
    "    # The retrieved documents are available under the 'context' key.\n",
    "    print(\"\\nAnswer:\")\n",
    "    # Use .get() for safe access in case 'answer' key is missing, although it should be present\n",
    "    print(response.get('answer', 'No answer generated.'))\n",
    "\n",
    "    # Optional: Also print retrieved context explicitly, as it's returned by create_retrieval_chain\n",
    "    # print(\"\\nRetrieved Context:\")\n",
    "    # if 'context' in response:\n",
    "    #     for i, doc in enumerate(response['context']):\n",
    "    #         print(f\"--- Document {i+1} ---\")\n",
    "    #         # Print snippet and metadata for the retrieved docs\n",
    "    #         print(doc.page_content[:200] + \"...\")\n",
    "    #         print(f\"Source: {doc.metadata.get('source')}, Page: {doc.metadata.get('page')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8723de20",
   "metadata": {},
   "source": [
    "Now that we already have a working pipeline to query our documents, we want to understand where our data is coming from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "59cf4f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up RAG Q&A with source citation instruction in the prompt.\n",
      "\n",
      "Setting up RAG chain for FAISS with top 4 matches.\n",
      "\n",
      "Executing the RAG chain with source citation instruction...\n",
      "\n",
      "Answer:\n",
      "The authors of the article \"Attention is all you need\" are Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. (Source: Attention Is All You Need, p. 1747-1756)\n"
     ]
    }
   ],
   "source": [
    "# Explicitly Stating Information Sources (Modern RAG with Source Prompt)\n",
    "\n",
    "\n",
    "print(\"Setting up RAG Q&A with source citation instruction in the prompt.\")\n",
    "\n",
    "# 1. Define a query of interest.\n",
    "query = \"Can you please tell me all the autors of the article Attention is all you need?\" # <--- Your search query\n",
    "\n",
    "# We can define how many similar results we want to get back by defining the variable k\n",
    "k_value = 4 # Recommend k=4 or more for RAG, original used k=1 which might not provide enough context\n",
    "\n",
    "# Ensure db_FAISS exists and is ready for use\n",
    "if 'db_FAISS' not in locals() or db_FAISS is None:\n",
    "    print(\"Error: FAISS vector store (db_FAISS) not found. Please run the cell creating db_FAISS first.\")\n",
    "elif 'chatgpt' not in locals() or chatgpt is None:\n",
    "     print(\"Error: ChatOpenAI model (chatgpt) not found. Please run a cell initializing chatgpt first.\")\n",
    "else:\n",
    "    print(f\"\\nSetting up RAG chain for FAISS with top {k_value} matches.\")\n",
    "\n",
    "    # Get a retriever instance from the FAISS database, configured with k\n",
    "    retriever = db_FAISS.as_retriever(search_kwargs={\"k\": k_value})\n",
    "\n",
    "    # --- Replacing load_qa_chain and manual metadata formatting ---\n",
    "\n",
    "    # 2. Define the Prompt Template for the LLM with Source Citation Instruction\n",
    "    # We modify the system prompt to explicitly ask the model to cite sources.\n",
    "    # The combine_docs_chain will provide the context including metadata (usually formatted).\n",
    "    rag_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Answer the user's question based ONLY on the below context.\n",
    "Each piece of context comes from a specific source document and page.\n",
    "If the answer is found in the context, provide the answer and ALSO state the source (PDF file name and page number) where you found the information.\n",
    "If the answer is not in the context, state that you cannot answer based on the provided information.\n",
    "\n",
    "Context:\n",
    "{context}\"\"\"), # The combine_docs_chain populates {context}\n",
    "        (\"user\", \"{question}\")\n",
    "    ])\n",
    "\n",
    "    # 3. Create the Chain to Combine Documents and Answer using create_stuff_documents_chain\n",
    "    # This utility handles formatting the list of Documents (including metadata) into a string for the prompt's {context}.\n",
    "    # It expects 'context' (list of Docs) and 'question' (string) as input.\n",
    "    # (Import create_stuff_documents_chain is already in Cell 1)\n",
    "    combine_docs_chain = create_stuff_documents_chain(\n",
    "        chatgpt,           # Pass your Chat LLM instance\n",
    "        rag_prompt_template # Pass the RAG prompt template with source instruction\n",
    "    )\n",
    "\n",
    "    # 4. Create the Full RAG Chain using create_retrieval_chain\n",
    "    # This utility connects the retriever and the combine_docs_chain.\n",
    "    # It takes {\"question\": ...} as input, runs the retriever, gets context, and passes both to the combine chain.\n",
    "    # (Import create_retrieval_chain is already in Cell 1)\n",
    "    rag_chain = create_retrieval_chain(\n",
    "        retriever,          # The retriever gets the relevant documents from FAISS\n",
    "        combine_docs_chain  # The combine_docs_chain uses the documents and question to answer\n",
    "    )\n",
    "\n",
    "    print(\"\\nExecuting the RAG chain with source citation instruction...\")\n",
    "\n",
    "    # 5. Execute the chain to obtain a response.\n",
    "    # Invoke the RAG chain with the query.\n",
    "    # create_retrieval_chain expects {\"question\": ...} as input.\n",
    "    response = rag_chain.invoke({\n",
    "        \"input\": query,\n",
    "        \"question\": query\n",
    "        }) # <--- Using 'question' key\n",
    "\n",
    "\n",
    "    # The response object from create_retrieval_chain is a dictionary.\n",
    "    # The final answer is typically under the 'answer' key.\n",
    "    # The retrieved documents are available under the 'context' key.\n",
    "    print(\"\\nAnswer:\")\n",
    "    # Use .get() for safe access in case 'answer' key is missing, although it should be present\n",
    "    print(response.get('answer', 'No answer generated.'))\n",
    "\n",
    "    # Optional: Also print retrieved context explicitly, as it's returned by create_retrieval_chain\n",
    "    # print(\"\\nRetrieved Context:\")\n",
    "    # if 'context' in response:\n",
    "    #     for i, doc in enumerate(response['context']):\n",
    "    #         print(f\"--- Document {i+1} ---\")\n",
    "    #         # Print snippet and metadata for the retrieved docs\n",
    "    #         print(doc.page_content[:200] + \"...\")\n",
    "    #         print(f\"Source: {doc.metadata.get('source')}, Page: {doc.metadata.get('page')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deeb5ff",
   "metadata": {},
   "source": [
    "Try to ask the model something that is completely out of scope, and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4dde4c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG Q&A with an out-of-scope query.\n",
      "\n",
      "Using FAISS for top 1 matches for out-of-scope query: 'What are the main problems to cook with olive oil?'\n",
      "\n",
      "Executing the RAG chain with the out-of-scope query...\n",
      "\n",
      "Answer:\n",
      "I cannot answer that based on the provided information.\n",
      "\n",
      "Retrieved Context:\n",
      "--- Document 1 ---\n",
      "bustness failure. If the subtly buggy code is sufﬁciently\n",
      "out-of-distribution, we might observe that the model per-\n",
      "forms worse in these cases, simply because it is thrown off\n",
      "by the OOD input - it is...\n",
      "Source: Docs\\codex.pdf, Page: 26\n"
     ]
    }
   ],
   "source": [
    "# Cell X: Testing RAG Q&A with an Out-of-Scope Query\n",
    "\n",
    "# Remove the outdated import for load_qa_chain (already removed in previous cell's update)\n",
    "# from langchain.chains.Youtubeing import load_qa_chain\n",
    "\n",
    "# Ensure `db_FAISS` (FAISS vector store instance) is created from a previous cell\n",
    "# Ensure `chatgpt` or `llm` (ChatOpenAI instance) is initialized from a previous cell\n",
    "# Ensure necessary imports for modern chains (from Cell 1) are available (already imported)\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain.chains import create_retrieval_chain\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "# from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print(\"Testing RAG Q&A with an out-of-scope query.\")\n",
    "\n",
    "# 1. Define a query of interest (out of scope).\n",
    "query = \"What are the main problems to cook with olive oil?\" # <--- Your out-of-scope query\n",
    "\n",
    "# We can define how many similar results we want to get back by defining the variable k\n",
    "# Using k=1 as in the original code, though k=4 or higher is generally recommended for RAG\n",
    "k_value = 1\n",
    "\n",
    "# Ensure db_FAISS exists and is ready for use\n",
    "if 'db_FAISS' not in locals() or db_FAISS is None:\n",
    "    print(\"Error: FAISS vector store (db_FAISS) not found. Please run the cell creating db_FAISS first.\")\n",
    "elif 'chatgpt' not in locals() or chatgpt is None:\n",
    "     print(\"Error: ChatOpenAI model (chatgpt) not found. Please run a cell initializing chatgpt first.\")\n",
    "else:\n",
    "    print(f\"\\nUsing FAISS for top {k_value} matches for out-of-scope query: '{query}'\")\n",
    "\n",
    "    # Get a retriever instance from the FAISS database, configured with k\n",
    "    # Note: Even for out-of-scope queries, the retriever will return *something* -\n",
    "    # the documents it considers *least dissimilar* if no truly similar ones exist.\n",
    "    retriever = db_FAISS.as_retriever(search_kwargs={\"k\": k_value})\n",
    "\n",
    "\n",
    "    # --- Reusing the modern LCEL-based RAG Chain setup ---\n",
    "    # The chain structure is the same as in the previous RAG Q&A cell (Step 1 in this section)\n",
    "    # The prompt already includes the instruction to state if the answer is not in the context.\n",
    "\n",
    "    # Define the Prompt Template for the LLM (same as previous RAG cells)\n",
    "    # This prompt includes the instruction to state if the answer is not in the context.\n",
    "    rag_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Answer the user's question based ONLY on the below context.\n",
    "Each piece of context comes from a specific source document and page.\n",
    "If the answer is found in the context, provide the answer and ALSO state the source (PDF file name and page number) where you found the information.\n",
    "If the answer is not in the context, state that you cannot answer based on the provided information.\n",
    "\n",
    "Context:\n",
    "{context}\"\"\"),\n",
    "        (\"user\", \"{question}\")\n",
    "    ])\n",
    "\n",
    "    # Create the Chain to Combine Documents and Answer\n",
    "    combine_docs_chain = create_stuff_documents_chain(\n",
    "        chatgpt,           # Pass your Chat LLM instance\n",
    "        rag_prompt_template # Pass the RAG prompt template\n",
    "    )\n",
    "\n",
    "    # Create the Full RAG Chain\n",
    "    rag_chain = create_retrieval_chain(\n",
    "        retriever,          # The retriever gets the relevant documents\n",
    "        combine_docs_chain  # The combine_docs_chain uses the documents and question to answer\n",
    "    )\n",
    "\n",
    "    print(\"\\nExecuting the RAG chain with the out-of-scope query...\")\n",
    "\n",
    "    # 5. Execute the chain to obtain a response.\n",
    "    # Invoke the RAG chain with the query.\n",
    "    response = rag_chain.invoke({\n",
    "            \"input\":   query,\n",
    "            \"question\": query\n",
    "        }) # <--- Using 'question' key\n",
    "\n",
    "\n",
    "    # The response object from create_retrieval_chain is a dictionary.\n",
    "    # The final answer is typically under the 'answer' key.\n",
    "    print(\"\\nAnswer:\")\n",
    "    print(response.get('answer', 'No answer generated.'))\n",
    "\n",
    "    # Optional: Also print retrieved context to see what was actually retrieved\n",
    "    print(\"\\nRetrieved Context:\")\n",
    "    if 'context' in response:\n",
    "        for i, doc in enumerate(response['context']):\n",
    "            print(f\"--- Document {i+1} ---\")\n",
    "            print(doc.page_content[:200] + \"...\")\n",
    "            print(f\"Source: {doc.metadata.get('source')}, Page: {doc.metadata.get('page')}\")\n",
    "    else:\n",
    "        print(\"No context returned in the response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0def231a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstrating semantic search with and without scores in FAISS.\n",
      "\n",
      "Searching FAISS for top 4 matches for query: 'Who went to mars?'\n",
      "Found 4 matches using similarity_search (without scores).\n",
      "First match (without score):\n",
      "  Source: Docs\\incoder.pdf, Page: 11\n",
      "  Content snippet: arXiv:2203.13474, 2022.\n",
      "12...\n",
      "\n",
      "Searching FAISS for top 4 matches for query: 'Who went to mars?' with scores\n",
      "Found 4 matches using similarity_search_with_score.\n",
      "Top matches with scores:\n",
      "\n",
      "--- Match 1 ---\n",
      "  Score: 0.5270625948905945\n",
      "  Source: Docs\\incoder.pdf, Page: 11\n",
      "  Content snippet: arXiv:2203.13474, 2022.\n",
      "12...\n",
      "\n",
      "--- Match 2 ---\n",
      "  Score: 0.5306929349899292\n",
      "  Source: Docs\\attentions.pdf, Page: 10\n",
      "  Content snippet: Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
      "arXiv:1703.03130, 2017.\n",
      "[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Mu...\n",
      "\n",
      "--- Match 3 ---\n",
      "  Score: 0.5419189929962158\n",
      "  Source: Docs\\codex.pdf, Page: 14\n",
      "  Content snippet: Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell,\n",
      "A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T.,\n",
      "Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse,\n",
      "C., Chen,...\n",
      "\n",
      "--- Match 4 ---\n",
      "  Score: 0.5422231554985046\n",
      "  Source: Docs\\codegen.pdf, Page: 0\n",
      "  Content snippet: Xiong (cxiong@salesforce.com).\n",
      "1\n",
      "arXiv:2203.13474v5  [cs.LG]  27 Feb 2023...\n"
     ]
    }
   ],
   "source": [
    "# Cell X: Performing Semantic Search with Similarity Scores (FAISS)\n",
    "\n",
    "print(\"Demonstrating semantic search with and without scores in FAISS.\")\n",
    "\n",
    "# 1. Define a query of interest.\n",
    "query = \"Who went to mars?\" # <--- search query (out of scope example)\n",
    "\n",
    "# We can define how many similar results we want to get back by defining the variable k\n",
    "k_value = 4 # Recommend specifying k for consistency, original used default for score search\n",
    "\n",
    "# Ensure db_FAISS exists and is ready for use\n",
    "if 'db_FAISS' not in locals() or db_FAISS is None:\n",
    "    print(\"Error: FAISS vector store (db_FAISS) not found. Please run the cell creating db_FAISS first.\")\n",
    "else:\n",
    "    print(f\"\\nSearching FAISS for top {k_value} matches for query: '{query}'\")\n",
    "\n",
    "    # 2. Perform a similarity search (without scores)\n",
    "    # This returns a list of Document objects.\n",
    "    matches = db_FAISS.similarity_search(query, k=k_value)\n",
    "    print(f\"Found {len(matches)} matches using similarity_search (without scores).\")\n",
    "    if matches:\n",
    "         print(\"First match (without score):\")\n",
    "         print(f\"  Source: {matches[0].metadata.get('source')}, Page: {matches[0].metadata.get('page')}\")\n",
    "         print(f\"  Content snippet: {matches[0].page_content[:100]}...\")\n",
    "\n",
    "\n",
    "    print(f\"\\nSearching FAISS for top {k_value} matches for query: '{query}' with scores\")\n",
    "\n",
    "    # 3. Perform a similarity search with scores\n",
    "    # This returns a list of tuples, where each tuple is (Document, score).\n",
    "    matches_and_scores = db_FAISS.similarity_search_with_score(query, k=k_value) # Specify k here too\n",
    "\n",
    "    print(f\"Found {len(matches_and_scores)} matches using similarity_search_with_score.\")\n",
    "\n",
    "    # Print the retrieved document chunks and their scores\n",
    "    if matches_and_scores:\n",
    "        print(\"Top matches with scores:\")\n",
    "        for i, (match, score) in enumerate(matches_and_scores): # Unpack the tuple\n",
    "            print(f\"\\n--- Match {i+1} ---\")\n",
    "            print(f\"  Score: {score}\") # Print the score\n",
    "            print(f\"  Source: {match.metadata.get('source')}, Page: {match.metadata.get('page', 'N/A')}\") # Access Document attributes\n",
    "            print(f\"  Content snippet: {match.page_content[:200]}...\") # Access Document attributes\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"No matches found in FAISS search with scores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79cdab59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2203.13474, 2022.\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(matches[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5473c1f9",
   "metadata": {},
   "source": [
    "Try other queries and talk with your documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2947a479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining the asking_your_model function (updated with internal check).\n"
     ]
    }
   ],
   "source": [
    "# Cell X: Encapsulating Basic RAG Q&A in a Function (FAISS) - Debugging Check\n",
    "\n",
    "# Ensure necessary imports for modern chains (from Cell 1) are available\n",
    "# Ensure necessary imports for FAISS and Embeddings (from Cell 1 or 15) are available\n",
    "\n",
    "print(\"Defining the asking_your_model function (updated with internal check).\")\n",
    "\n",
    "# Define the asking_your_model function\n",
    "# This function encapsulates the RAG Q&A logic using the FAISS index.\n",
    "# It takes the query string and k (number of results) as input.\n",
    "# It assumes db_FAISS and chatgpt (or llm) are accessible in the scope where the function is called.\n",
    "def asking_your_model(query: str, k: int):\n",
    "    \"\"\"\n",
    "    Performs RAG Q&A using the FAISS vector store.\n",
    "\n",
    "    Args:\n",
    "        query: The user's question.\n",
    "        k: The number of similar documents to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the 'answer' and 'context' from the RAG chain.\n",
    "    \"\"\"\n",
    "    # --- Debugging: Check db_FAISS state inside the function ---\n",
    "    print(\"--- Debugging inside function ---\")\n",
    "    # Check if 'db_FAISS' exists in the global scope (where notebook variables live)\n",
    "    if 'db_FAISS' in globals():\n",
    "         print(f\"db_FAISS FOUND in globals() inside function. Type: {type(globals()['db_FAISS'])}\")\n",
    "         # Optional: print a representation if you want more detail (can be verbose)\n",
    "         # print(f\"db_FAISS repr: {repr(globals()['db_FAISS'])}\")\n",
    "    else:\n",
    "         print(\"db_FAISS NOT found in globals() inside function.\")\n",
    "\n",
    "    # Check if 'chatgpt' exists in the global scope\n",
    "    if 'chatgpt' in globals():\n",
    "         print(f\"chatgpt FOUND in globals() inside function. Type: {type(globals()['chatgpt'])}\")\n",
    "    else:\n",
    "         print(\"chatgpt NOT found in globals() inside function.\")\n",
    "    print(\"--- End Debugging inside function ---\")\n",
    "\n",
    "\n",
    "    # Ensure necessary variables are available within the function scope\n",
    "    # Check in globals() instead of locals() for notebook-level variables\n",
    "    # Use globals()['variable_name'] to access them explicitly\n",
    "    if 'db_FAISS' not in globals() or globals()['db_FAISS'] is None:\n",
    "        print(\"Error: FAISS vector store (db_FAISS) not found.\")\n",
    "        return {\"answer\": \"Error: Vector store not initialized.\", \"context\": []}\n",
    "    # Also check chatgpt for completeness\n",
    "    if 'chatgpt' not in globals() or globals()['chatgpt'] is None:\n",
    "        print(\"Error: ChatOpenAI model (chatgpt) not found.\")\n",
    "        return {\"answer\": \"Error: LLM not initialized.\", \"context\": []}\n",
    "\n",
    "    # --- RAG Chain Logic (using globals() to access db_FAISS and chatgpt) ---\n",
    "\n",
    "    # 1. Get a Retriever from the FAISS Database\n",
    "    # Access db_FAISS using globals()['db_FAISS']\n",
    "    retriever = globals()['db_FAISS'].as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "\n",
    "    # 2. Define the Prompt Template for the LLM with Source Citation Instruction\n",
    "    # ... (same as before - prompt definition doesn't need globals()) ...\n",
    "    rag_prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Answer the user's question based ONLY on the below context.\n",
    "Each piece of context comes from a specific source document and page.\n",
    "If the answer is found in the context, provide the answer and ALSO state the source (PDF file name and page number) where you found the information.\n",
    "If the answer is not in the context, state that you cannot answer based on the provided information.\n",
    "\n",
    "Context:\n",
    "{context}\"\"\"),\n",
    "        (\"user\", \"{question}\")\n",
    "    ])\n",
    "\n",
    "\n",
    "    # 3. Create the Chain to Combine Documents and Answer using create_stuff_documents_chain\n",
    "    # Access chatgpt using globals()['chatgpt']\n",
    "    combine_docs_chain = create_stuff_documents_chain(\n",
    "        globals()['chatgpt'],           # Pass your Chat LLM instance\n",
    "        rag_prompt_template # Pass the RAG prompt template\n",
    "    )\n",
    "\n",
    "\n",
    "    # 4. Create the Full RAG Chain using create_retrieval_chain\n",
    "    # ... (same as before) ...\n",
    "    rag_chain = create_retrieval_chain(\n",
    "        retriever,          # The retriever gets the relevant documents from FAISS\n",
    "        combine_docs_chain  # The combine_docs_chain uses the documents and question to answer\n",
    "    )\n",
    "\n",
    "\n",
    "    # 5. Execute the chain to obtain a response.\n",
    "    # ... (same as before) ...\n",
    "    response = rag_chain.invoke({\n",
    "         \"input\":   query,\n",
    "         \"question\": query\n",
    "        })\n",
    "\n",
    "\n",
    "    # 6. Return the response\n",
    "    # ... (same as before) ...\n",
    "    return response\n",
    "\n",
    "\n",
    "# Optional: Example usage of the function (same as before)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "27505cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to call the asking_your_model function.\n",
      "--- Debugging before function call ---\n",
      "db_FAISS FOUND in globals() before function call. Type: <class 'langchain_community.vectorstores.faiss.FAISS'>\n",
      "chatgpt FOUND in globals() before function call. Type: <class 'langchain_openai.chat_models.base.ChatOpenAI'>\n",
      "--- End Debugging before function call ---\n",
      "\n",
      "Calling the asking_your_model function...\n",
      "--- Debugging inside function ---\n",
      "db_FAISS FOUND in globals() inside function. Type: <class 'langchain_community.vectorstores.faiss.FAISS'>\n",
      "chatgpt FOUND in globals() inside function. Type: <class 'langchain_openai.chat_models.base.ChatOpenAI'>\n",
      "--- End Debugging inside function ---\n",
      "\n",
      "Full response dictionary:\n",
      "{'input': 'What is functional correctness?', 'question': 'What is functional correctness?', 'context': [Document(id='a9fed2fb-232f-4501-acd2-e164b6417cc6', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Docs/attentions.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'text': 'Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'), Document(id='2843fc0b-d171-471c-b095-4415498665b5', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Docs/attentions.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'text': 'Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'), Document(id='ed4568b9-1f8e-4b46-9398-6faa23f819d9', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Docs/attentions.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'text': '[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'), Document(id='d52f806a-46e4-44a4-b51d-3d3472d0f33d', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Docs/attentions.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'text': 'Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13')], 'answer': 'I cannot answer that based on the provided information.'}\n",
      "\n",
      "Answer:\n",
      "I cannot answer that based on the provided information.\n"
     ]
    }
   ],
   "source": [
    "# Cell X: Calling the Encapsulated RAG Function (Debugging Call)\n",
    "\n",
    "# Ensure the asking_your_model function is defined from the previous cell\n",
    "# Ensure db_FAISS and chatgpt (or llm) are initialized from previous cells (as needed by the function)\n",
    "\n",
    "print(\"Attempting to call the asking_your_model function.\")\n",
    "\n",
    "# --- Debugging: Check db_FAISS state before the function call ---\n",
    "print(\"--- Debugging before function call ---\")\n",
    "if 'db_FAISS' in globals():\n",
    "    print(f\"db_FAISS FOUND in globals() before function call. Type: {type(globals()['db_FAISS'])}\")\n",
    "    # Optional: print a representation (be careful with large objects)\n",
    "    # print(f\"db_FAISS repr: {repr(globals()['db_FAISS'])}\")\n",
    "else:\n",
    "    print(\"db_FAISS NOT found in globals() before function call.\")\n",
    "\n",
    "if 'chatgpt' in globals():\n",
    "    print(f\"chatgpt FOUND in globals() before function call. Type: {type(globals()['chatgpt'])}\")\n",
    "else:\n",
    "    print(\"chatgpt NOT found in globals() before function call.\")\n",
    "\n",
    "print(\"--- End Debugging before function call ---\")\n",
    "\n",
    "\n",
    "# Define the query\n",
    "query = \"What is functional correctness?\" # <--- Your query\n",
    "\n",
    "# Define the number of documents to retrieve\n",
    "k_value = 4 # <--- Number of documents to retrieve\n",
    "\n",
    "# Call the asking_your_model function with the query and k\n",
    "print(\"\\nCalling the asking_your_model function...\")\n",
    "response = asking_your_model(query, k=k_value)\n",
    "\n",
    "# Print the response\n",
    "print(\"\\nFull response dictionary:\")\n",
    "print(response)\n",
    "\n",
    "# You can access the answer and context keys separately\n",
    "print(\"\\nAnswer:\")\n",
    "print(response.get('answer', 'No answer generated.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "80c21f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Debugging inside function ---\n",
      "db_FAISS FOUND in globals() inside function. Type: <class 'langchain_community.vectorstores.faiss.FAISS'>\n",
      "chatgpt FOUND in globals() inside function. Type: <class 'langchain_openai.chat_models.base.ChatOpenAI'>\n",
      "--- End Debugging inside function ---\n",
      "{'input': 'What is functional correctness?', 'question': 'What is functional correctness?', 'context': [Document(id='a9fed2fb-232f-4501-acd2-e164b6417cc6', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Docs/attentions.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'text': 'Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'), Document(id='2843fc0b-d171-471c-b095-4415498665b5', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Docs/attentions.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'text': 'Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'), Document(id='ed4568b9-1f8e-4b46-9398-6faa23f819d9', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Docs/attentions.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'text': '[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'), Document(id='d52f806a-46e4-44a4-b51d-3d3472d0f33d', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Docs/attentions.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'text': 'Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13')], 'answer': 'I cannot answer that based on the provided information.'}\n"
     ]
    }
   ],
   "source": [
    "# Check similarity search is working\n",
    "query = \"What is functional correctness?\"\n",
    "response = asking_your_model(query, k=4)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f730e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Debugging inside function ---\n",
      "db_FAISS FOUND in globals() inside function. Type: <class 'langchain_community.vectorstores.faiss.FAISS'>\n",
      "chatgpt FOUND in globals() inside function. Type: <class 'langchain_openai.chat_models.base.ChatOpenAI'>\n",
      "--- End Debugging inside function ---\n",
      "Multi-head attention in a transformer involves linearly projecting the queries, keys, and values multiple times with different learned linear projections to different dimensions. These projected versions are then used to perform the attention function in parallel, resulting in multiple sets of values. This allows the model to jointly attend to information from different representation subspaces at different positions. Each set of values is concatenated and projected again to produce the final values. This information can be found in the document \"Attention Is All You Need\" on page 4.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Check similarity search is working\n",
    "query = \"What is the multi-head attention in a transformer?\"\n",
    "response = asking_your_model(query, k=4)\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
